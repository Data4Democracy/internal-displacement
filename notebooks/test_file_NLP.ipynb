{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.metrics import classification_report\n",
    "from internal_displacement.interpreter import Interpreter\n",
    "from internal_displacement.excerpt_helper import Helper\n",
    "from internal_displacement.excerpt_helper import MeanEmbeddingVectorizer\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Set up necessary arguments for the Interpreter\n",
    "nlp = spacy.load('en')\n",
    "person_reporting_terms = [\n",
    "    'displaced', 'evacuated', 'forced', 'flee', 'homeless', 'relief camp',\n",
    "    'sheltered', 'relocated', 'stranded', 'stuck', 'accommodated']\n",
    "\n",
    "structure_reporting_terms = [\n",
    "    'destroyed', 'damaged', 'swept', 'collapsed',\n",
    "    'flooded', 'washed', 'inundated', 'evacuate'\n",
    "]\n",
    "\n",
    "person_reporting_units = [\"families\", \"person\", \"people\", \"individuals\", \"locals\", \"villagers\", \"residents\",\n",
    "                            \"occupants\", \"citizens\", \"households\"]\n",
    "\n",
    "structure_reporting_units = [\"home\", \"house\", \"hut\", \"dwelling\", \"building\"]\n",
    "\n",
    "relevant_article_terms = ['Rainstorm', 'hurricane',\n",
    "                          'tornado', 'rain', 'storm', 'earthquake']\n",
    "relevant_article_lemmas = [t.lemma_ for t in nlp(\n",
    "    \" \".join(relevant_article_terms))]\n",
    "\n",
    "data_path = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the interpreter\n",
    "interpreter = Interpreter(nlp, person_reporting_terms, structure_reporting_terms, person_reporting_units,\n",
    "                          structure_reporting_units, relevant_article_lemmas, data_path,\n",
    "                          model_path='../internal_displacement/classifiers/default_model.pkl',\n",
    "                          encoder_path='../internal_displacement/classifiers/default_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initializer the helper\n",
    "helper = Helper(nlp, '../internal_displacement/classifiers/unit_vectorizer.pkl', \n",
    "               '../internal_displacement/classifiers/unit_model.pkl',\n",
    "               '../internal_displacement/classifiers/term_vectorizer.pkl',\n",
    "               '../internal_displacement/classifiers/term_model.pkl',\n",
    "               '../internal_displacement/classifiers/terem_svc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained Word2Vec model\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Load the data and clean up the excerpts (removes text in brackets, irrelevant tokens etc.)\n",
    "\n",
    "test_df = pd.read_excel(\"../data/IDETECT_test_dataset - NLP.csv.xlsx\")\n",
    "test_df['cleaned_text'] = test_df['excerpt'].apply(lambda x: helper.cleanup(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Extract reports and choose the most likely one\n",
    "test_df['reports'] = test_df['cleaned_text'].apply(lambda x: interpreter.process_article_new(x))\n",
    "test_df['most_likely_report'] = test_df['reports'].apply(lambda x: helper.get_report(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1. Use the rules-based Interpreter\n",
    "test_df['unit_rules'] = test_df['most_likely_report'].apply(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2. Use the classifier\n",
    "X_test = helper.reporting_unit_vectorizer.transform(test_df['cleaned_text'])\n",
    "test_df['unit_clf'] = helper.reporting_unit_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3. Combine the predictions\n",
    "test_df['unit_combined'] = test_df[['unit_rules', 'unit_clf']].apply(lambda x: helper.combine_predictions(x['unit_clf'], x['unit_rules']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1. Use the rules-based Interpreter\n",
    "test_df['term_rules'] = test_df['most_likely_report'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2. Use the classifiers to get the probabilities and combine them into a single prediction\n",
    "w2vVectorizer = MeanEmbeddingVectorizer(w2v)\n",
    "X_feat_1 = helper.reporting_term_vectorizer.transform(test_df['cleaned_text'])\n",
    "p1 = helper.reporting_term_classifier.predict_proba(X_feat_1)\n",
    "X_feat_2 = w2vVectorizer.transform(test_df['cleaned_text'])\n",
    "p2 = helper.reporting_term_svc.predict_proba(X_feat_2)\n",
    "\n",
    "test_df['term_clf'] = helper.combine_probabilities(p1, p2, helper.reporting_term_classifier.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 3. Combine the predictions\n",
    "test_df['term_combined'] = test_df[['term_rules', 'term_clf']].apply(lambda x: helper.combine_predictions(x['term_clf'], x['term_rules']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "person_units = [\"person\", \"people\", \"individuals\", \"locals\", \"villagers\", \"residents\",\n",
    "                \"occupants\", \"citizens\", \"IDP\"]\n",
    "\n",
    "household_units = [\"home\", \"house\", \"hut\", \"dwelling\", \"building\", \"families\", \"households\"]\n",
    "\n",
    "person_lemmas =[t.lemma_ for t in nlp(\" \".join(person_units))]\n",
    "household_lemmas =[t.lemma_ for t in nlp(\" \".join(household_units))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1. Get quantity from top report\n",
    "test_df['quantity_rules_1'] = test_df['most_likely_report'].apply(lambda x: x[0])\n",
    "test_df['quantity_rules_1'] = test_df['quantity_rules_1'].fillna(0)\n",
    "test_df['quantity_rules_1'] = test_df['quantity_rules_1'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2. Get quantity using other rules\n",
    "test_df['quantity_rules_2'] = test_df[['excerpt', 'unit_combined']].apply(lambda x: helper.get_number(x['excerpt'], x['unit_combined'], person_lemmas, household_lemmas), axis=1)\n",
    "test_df['quantity_rules_2'] = test_df['quantity_rules_2'].fillna(0)\n",
    "test_df['quantity_rules_2'] = test_df['quantity_rules_2'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df['quantity_combined'] = test_df[['quantity_rules_1', 'quantity_rules_2']].apply(lambda x: helper.combine_quantities(x['quantity_rules_1'], x['quantity_rules_2']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location & Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df['locations'] = test_df['excerpt'].apply(lambda x: interpreter.extract_countries(interpreter.cleanup(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df['top_location'] = test_df['locations'].apply(lambda x: helper.choose_country(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df['loc_name'] = test_df['top_location'].apply(lambda x: x[0])\n",
    "test_df['country_code'] = test_df['top_location'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_df = test_df[['excerpt_id', 'excerpt', 'unit_combined', 'term_combined', \n",
    "       'quantity_combined', 'loc_name', 'country_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_df.columns = ['excerpt_id', 'excerpt', 'reporting_unit', 'reporting_term', \n",
    "       'quantity', 'location_name', 'country_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_df.to_csv('../data/test_NLP_output.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
