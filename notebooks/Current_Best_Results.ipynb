{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a summary of the current 'best' approaches for extracting the relevant terms as requested by IDMC:\n",
    "- Reporting term\n",
    "- Reporting unit\n",
    "- Displacement figure\n",
    "- Location & Country\n",
    "\n",
    "For reference we have:\n",
    "1. Set of ~130 pre-labelled examples provided by IDMC (~118 in English)\n",
    "2. Test set of excerpts that have been hand-labelled\n",
    "\n",
    "Note: in many cases the relevant labels are quite subjective given the vagaries of language; furthermore in some cases multiple 'terms' or 'units' can be mentioned in the same sentence or excerpt; similarly there can often be ambiguous locations within the text\n",
    "\n",
    "In general, in order to choose the most likely or relevant terms from the text, we are using the following heuristics:\n",
    "\n",
    "1. Things affecting 'People' take precedence over things affecting Structures\n",
    "2. Where clarification is given in brackets it is ignored (i.e., 1,000 families (3,000 people) should give 1,000 families)\n",
    "3. Reports of destroyed structures take precedence over reports of damaged structures\n",
    "4. If there is still ambiguity, the first possible report is chosen\n",
    "5. Where multiple possible countries are mentioned:\n",
    "    - The most common country is chosen, or\n",
    "    - The first country is chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting Unit: People or Households\n",
    "\n",
    "Overall Approach: Combine output of rules-based and machine learning model\n",
    "\n",
    "Machine Learning Model: Multinomial Naive Bayes\n",
    "Optimized based on ~130 training examples provided\n",
    "Grid Search for Best Alpha parameter\n",
    "\n",
    "Text Pre-Processing:\n",
    "    \n",
    "1. Remove named entities (People, Dates, Locations, Organizations, Numbers)\n",
    "2. Remove text in brackets\n",
    "3. Clean unusual characters etc.\n",
    "\n",
    "Features:\n",
    "    \n",
    "Words extracted using count vectorizer; stop words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import sklearn.pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from internal_displacement.interpreter import Interpreter\n",
    "from internal_displacement.extracted_report import convert_quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "person_reporting_terms = [\n",
    "    'displaced', 'evacuated', 'forced', 'flee', 'homeless', 'relief camp',\n",
    "    'sheltered', 'relocated', 'stranded', 'stuck', 'accommodated']\n",
    "\n",
    "structure_reporting_terms = [\n",
    "    'destroyed', 'damaged', 'swept', 'collapsed',\n",
    "    'flooded', 'washed', 'inundated', 'evacuate'\n",
    "]\n",
    "\n",
    "person_reporting_units = [\"families\", \"person\", \"people\", \"individuals\", \"locals\", \"villagers\", \"residents\",\n",
    "                            \"occupants\", \"citizens\", \"households\"]\n",
    "\n",
    "structure_reporting_units = [\"home\", \"house\", \"hut\", \"dwelling\", \"building\"]\n",
    "\n",
    "relevant_article_terms = ['Rainstorm', 'hurricane',\n",
    "                          'tornado', 'rain', 'storm', 'earthquake']\n",
    "relevant_article_lemmas = [t.lemma_ for t in nlp(\n",
    "    \" \".join(relevant_article_terms))]\n",
    "\n",
    "data_path = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interpreter = Interpreter(nlp, person_reporting_terms, structure_reporting_terms, person_reporting_units,\n",
    "                          structure_reporting_units, relevant_article_lemmas, data_path,\n",
    "                          model_path='../internal_displacement/classifiers/default_model.pkl',\n",
    "                          encoder_path='../internal_displacement/classifiers/default_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the training data, remove non-English excerpts\n",
    "df = pd.read_csv(\"../data/IDMC_fully_labelled.csv\", encoding='latin1')\n",
    "df['lang'] = df['Excerpt'].apply(lambda x: interpreter.check_language(x))\n",
    "df = df[df['lang'] == 'en'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_brackets(text):\n",
    "    '''Remove items in brackets\n",
    "    '''\n",
    "    text = re.sub(r'(\\(.+\\))', '', text)\n",
    "    text = re.sub(r'\\s{2}', ' ', text)\n",
    "    return text\n",
    "    \n",
    "def cleanup(text):\n",
    "        '''Clean common errors in the text and remove irrelevant tokens and stop words'''\n",
    "        text = re.sub(r'([a-zA-Z0-9])(IMPACT)', r'\\1. \\2', text)\n",
    "        text = re.sub(r'([a-zA-Z0-9])(RESPONSE)', r'\\1. \\2', text)\n",
    "        text = re.sub(r'(IMPACT)([a-zA-Z0-9])', r'\\1. \\2', text)\n",
    "        text = re.sub(r'(RESPONSE)([a-zA-Z0-9])', r'\\1. \\2', text)\n",
    "        text = re.sub(r'([a-zA-Z])(\\d)', r'\\1. \\2', text)\n",
    "        text = re.sub(r'(\\d)\\s(\\d)', r'\\1\\2', text)\n",
    "        text = text.replace('\\r', ' ')\n",
    "        text = text.replace('  ', ' ')\n",
    "        text = text.replace(\"peole\", \"people\")\n",
    "        output = ''\n",
    "        for char in text:\n",
    "            if char in string.printable:\n",
    "                output += char\n",
    "        output = remove_irrelevant_tokens(output)\n",
    "        return output\n",
    "\n",
    "def remove_irrelevant_tokens(text):\n",
    "    '''Remove phrases in brackets and irrelevant tokens (named entities and stop words)\n",
    "    Return lemmatized text'''\n",
    "    text = remove_brackets(text)\n",
    "    output = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if test_token(token):\n",
    "            output.append(token)\n",
    "    return \" \".join([t.lemma_ for t in output])\n",
    "    \n",
    "def test_token(token):\n",
    "    '''Test tokens for exclusion'''\n",
    "    if token.like_num:\n",
    "        return False\n",
    "    elif token.ent_type_ in ('LOC', 'GPE', 'PERSON', 'ORG', 'DATE', 'FAC', 'NORP'):\n",
    "        return False\n",
    "    elif token.is_stop:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[['Excerpt', 'Reporting unit']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clean excerpts using functions above\n",
    "df['cleaned_text'] = df['Excerpt'].apply(lambda x: cleanup(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use grid-search to optimize for MultiNomial Naive Bayes alpha param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Words as features\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", binary = True, \n",
    "                             ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['Reporting unit'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [0, 0.1, 1, 10]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = dict(alpha=[0, 0.1, 1, 10])\n",
    "cv = GridSearchCV(clf, param_grid=parameters)\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(X_test)\n",
    "y_predictions = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      " Households       1.00      0.50      0.67         6\n",
      "     People       0.86      1.00      0.92        18\n",
      "\n",
      "avg / total       0.89      0.88      0.86        24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report( y_test, y_predictions ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Train the classifier on all the training data\n",
    "clf = MultinomialNB(alpha=1)\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", binary = True, \n",
    "                             ngram_range=(1,1))\n",
    "X_train = vectorizer.fit_transform(df['cleaned_text'])\n",
    "y_train = df['Reporting unit']\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run tests on the actual test data;\n",
    "\n",
    "Note, this data is hand tagged which in many cases is subjective\n",
    "There may be errors, or the tags may not agree with IDMC defined-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the data and perform the same cleanup procedure\n",
    "test_df = pd.read_excel(\"../data/IDETECT_test_dataset - NLP.csv.xlsx\")\n",
    "test_df['cleaned_text'] = test_df['excerpt'].apply(lambda x: cleanup(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1: Results of only using the Multinomial NB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(test_df['cleaned_text'])\n",
    "y_test = test_df['Unit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      " Households       0.75      0.24      0.37        37\n",
      "     People       0.91      0.99      0.95       275\n",
      "\n",
      "avg / total       0.89      0.90      0.88       312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_y = clf.predict(X_test)\n",
    "print(classification_report( y_test, predicted_y ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2: Results of only using the hand-crafted rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimum_loc(spans):\n",
    "    '''Find the first character location in text for each report\n",
    "    '''\n",
    "    locs = []\n",
    "    for s in spans:\n",
    "        if s['type'] != 'loc':\n",
    "            locs.append(s['start'])\n",
    "    return min(locs)\n",
    "\n",
    "def choose_report(reports):\n",
    "    '''Choose report based on the heuristics mentioned in the first cell\n",
    "    '''\n",
    "    people_reports = []\n",
    "    household_reports_1 = []\n",
    "    household_reports_2 = []\n",
    "    \n",
    "    for r in reports:\n",
    "        if r.subject_term == \"People\":\n",
    "            people_reports.append(r)\n",
    "        elif r.subject_term == \"Households\":\n",
    "            if r.event_term in (\"Partially Destroyed Housing\", \"Uninhabitable Housing\"):\n",
    "                household_reports_2.append(r)\n",
    "            else:\n",
    "                household_reports_1.append(r)\n",
    "    if len(people_reports) > 0:\n",
    "        report = first_report(people_reports)\n",
    "    elif len(household_reports_1) > 0:\n",
    "        report = first_report(household_reports_1)\n",
    "    elif len(household_reports_2) > 0:\n",
    "        report = first_report(household_reports_2)\n",
    "    else:\n",
    "        report = reports[0]\n",
    "    \n",
    "    return report\n",
    "    \n",
    "\n",
    "def first_report(reports):\n",
    "    '''Choose the first report based on location in text'''\n",
    "    report_locs = []\n",
    "    for report in reports:\n",
    "        report_locs.append((report, minimum_loc(report.tag_spans)))\n",
    "    return sorted(report_locs, key=lambda x: x[1])[0][0]\n",
    "\n",
    "def get_report(excerpt):\n",
    "    '''Get reports based on Excerpt and choose the most relevant one'''\n",
    "    reports = interpreter.process_article_new(excerpt)\n",
    "    if len(reports) > 0:\n",
    "        report = choose_report(reports)\n",
    "        return report.quantity, report.event_term, report.subject_term, report.locations\n",
    "    else:\n",
    "        return 0, '', '', ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try and clean the text and remove brackets\n",
    "test_df['cleaned_excerpt'] = test_df['excerpt'].apply(lambda x: cleanup(remove_brackets(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract the report\n",
    "test_df['extracted_report'] = test_df['cleaned_excerpt'].apply(lambda x: get_report(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the predicted unit from the extracted report\n",
    "test_df['predicted_unit'] = test_df['extracted_report'].apply(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "                  0.00      0.00      0.00         0\n",
      " Households       1.00      0.32      0.49        37\n",
      "     People       0.97      0.63      0.76       275\n",
      "\n",
      "avg / total       0.97      0.60      0.73       312\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report( y_test, test_df['predicted_unit'] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 3: Combine classifier and hand-crafted results into single prediction\n",
    "1. If they both agree -> done\n",
    "2. If no rules-based prediction -> use classifier result\n",
    "3. If both predictions exist but they differ -> use hand-crafted rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_predictions(classifier, rules):\n",
    "    if classifier == rules:\n",
    "        return classifier\n",
    "    elif not rules or rules == '':\n",
    "        return classifier\n",
    "    else:\n",
    "        return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_predictions = []\n",
    "for p1, p2 in zip(predicted_y, test_df['predicted_unit']):\n",
    "    combined_predictions.append(combine_predictions(p1, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      " Households       0.90      0.51      0.66        37\n",
      "     People       0.94      0.99      0.96       275\n",
      "\n",
      "avg / total       0.93      0.94      0.93       312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report( y_test, combined_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Country extraction approach is currently based on Spacy entity extraction and hand-crafted rules for translated extracted locations into 3-digit ISO country codes\n",
    "\n",
    "If more than one country is identified in the fragment, the rules for choosing the country are:\n",
    "\n",
    "1. Country that occurs the highest number of times or,\n",
    "2. Country that appears first in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on the provided pre-labelled training data\n",
    "\n",
    "Note: in some cases a country code is still given, even though there are no locations mentioned in the excerpt\n",
    "We ignore these cases for testing purposes (the information likely comes from a different source that will not\n",
    "be included in the specific NLP test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the training data, remove non-English excerpts\n",
    "df = pd.read_excel(\"../data/IDMC_fully_labelled.csv.xlsx\", encoding='latin1')\n",
    "df['lang'] = df['Excerpt'].apply(lambda x: interpreter.check_language(x))\n",
    "df = df[df['lang'] == 'en'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Remove those lines where no location is mentioned\n",
    "df = df[df['Mentions'] == 'Y'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_country_code(text):\n",
    "    '''Extract country code from ; separated values'''\n",
    "    return text.split(';')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['true_code'] = df['Location (Country)'].apply(lambda x: get_country_code(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_country(countries, first_country=''):\n",
    "    '''Choose country out of possible countries\n",
    "    Either return the most commonly mentioned country\n",
    "    or the first country found\n",
    "    '''\n",
    "    \n",
    "    if len(countries) == 0:\n",
    "        return ''\n",
    "    if len(countries) <= 1:\n",
    "        return countries.most_common()[0][0]\n",
    "    else:\n",
    "        country_counts = list(countries.values())\n",
    "        max_count = max(country_counts)\n",
    "        if country_counts.count(max_count) > 1:\n",
    "            return first_country\n",
    "        else:\n",
    "            return countries.most_common()[0][0]\n",
    "\n",
    "def get_location(excerpt):\n",
    "    '''Extract the country from a text fragment by\n",
    "    identifying all possible countries and selecting\n",
    "    from among them\n",
    "    '''\n",
    "    excerpt = interpreter.cleanup(excerpt)\n",
    "    possible_countries, first_country = interpreter.extract_countries(excerpt)\n",
    "    chosen_country = choose_country(possible_countries, first_country)\n",
    "    return chosen_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['extracted_code'] = df['Excerpt'].apply(lambda x: get_location(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1, support = sklearn.metrics.precision_recall_fscore_support( df['true_code'], df['extracted_code'], average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8293752283522104, Recall: 0.782608695652174, F1: 0.8009130965652705, Support: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: {}, Recall: {}, F1: {}, Support: {}\".format(precision, recall, f1, support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on the self-labelled testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df['extracted_code'] = test_df['excerpt'].apply(lambda x: get_location(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1, support = sklearn.metrics.precision_recall_fscore_support(test_df['Country'].fillna(''), test_df['extracted_code'], average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8543503949753949, Recall: 0.7596153846153846, F1: 0.7767415657104066, Support: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: {}, Recall: {}, F1: {}, Support: {}\".format(precision, recall, f1, support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reporting term can be one of 10 categories (i.e. Displaced, Evacuated, In Relief Camp etc)\n",
    "\n",
    "A similar approach is taken as per reporting unit (i.e. combining outcome of hand-crafted rules & classifier).\n",
    "\n",
    "In this case, the classifier is even harder to train as many of the categories have very few or 0 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the training data, remove non-English excerpts\n",
    "df = pd.read_excel(\"../data/IDMC_fully_labelled.csv.xlsx\", encoding='latin1')\n",
    "df['lang'] = df['Excerpt'].apply(lambda x: interpreter.check_language(x))\n",
    "df = df[df['lang'] == 'en'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[['Excerpt', 'Reporting term']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['Excerpt'].apply(lambda x: cleanup(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", binary = True, \n",
    "                             ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['Reporting term'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [0, 0.1, 1, 10]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = dict(alpha=[0, 0.1, 1, 10])\n",
    "cv = GridSearchCV(clf, param_grid=parameters)\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(X_test)\n",
    "y_predictions = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Destroyed Housing       0.00      0.00      0.00         2\n",
      "        Displaced       1.00      0.80      0.89         5\n",
      "        Evacuated       0.71      0.92      0.80        13\n",
      "   Forced to Flee       0.00      0.00      0.00         1\n",
      "         Homeless       0.00      0.00      0.00         1\n",
      "   In Relief Camp       0.00      0.00      0.00         1\n",
      "        Relocated       1.00      1.00      1.00         1\n",
      "        Sheltered       0.00      0.00      0.00         1\n",
      "\n",
      "      avg / total       0.61      0.68      0.63        25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report( y_test, y_predictions ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Train the classifier on all the training data\n",
    "clf = MultinomialNB(alpha=1)\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", binary = True, \n",
    "                             ngram_range=(2,2))\n",
    "X_train = vectorizer.fit_transform(df['cleaned_text'])\n",
    "y_train = df['Reporting term']\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run tests on the actual test data;\n",
    "\n",
    "Note, this data is hand tagged which in many cases is subjective\n",
    "There may be errors, or the tags may not agree with IDLT defined-tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1: Results of only using the Multinomial NB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(test_df['cleaned_text'])\n",
    "y_test = test_df['Term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_y = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             precision    recall  f1-score   support\n",
      "\n",
      "          Destroyed Housing       0.44      0.40      0.42        10\n",
      "                  Displaced       0.93      0.68      0.79       182\n",
      "                  Evacuated       0.26      0.96      0.41        46\n",
      "             Forced to Flee       0.00      0.00      0.00        39\n",
      "                   Homeless       1.00      0.50      0.67         2\n",
      "             In Relief Camp       0.00      0.00      0.00         8\n",
      "Partially Destroyed Housing       0.00      0.00      0.00         2\n",
      "                  Relocated       0.00      0.00      0.00         5\n",
      "                  Sheltered       0.00      0.00      0.00        18\n",
      "\n",
      "                avg / total       0.60      0.55      0.54       312\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report( y_test, predicted_y ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2: Results of only using the hand crafted rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df['predicted_term'] = test_df['extracted_report'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                                  0.00      0.00      0.00         0\n",
      "          Destroyed Housing       1.00      0.20      0.33        10\n",
      "                  Displaced       0.95      0.66      0.78       182\n",
      "                  Evacuated       1.00      0.70      0.82        46\n",
      "             Forced to Flee       0.81      0.64      0.71        39\n",
      "                   Homeless       0.00      0.00      0.00         2\n",
      "             In Relief Camp       0.00      0.00      0.00         8\n",
      "Partially Destroyed Housing       0.00      0.00      0.00         2\n",
      "                  Relocated       1.00      0.20      0.33         5\n",
      "                  Sheltered       0.00      0.00      0.00        18\n",
      "\n",
      "                avg / total       0.85      0.58      0.68       312\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report( y_test, test_df['predicted_term'] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 3: Results of combining classifier predictions and hand crafted rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_predictions = []\n",
    "for p1, p2 in zip(predicted_y, test_df['predicted_term']):\n",
    "    combined_predictions.append(combine_predictions(p1, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             precision    recall  f1-score   support\n",
      "\n",
      "          Destroyed Housing       0.57      0.40      0.47        10\n",
      "                  Displaced       0.94      0.74      0.82       182\n",
      "                  Evacuated       0.34      0.96      0.50        46\n",
      "             Forced to Flee       0.81      0.64      0.71        39\n",
      "                   Homeless       1.00      0.50      0.67         2\n",
      "             In Relief Camp       0.00      0.00      0.00         8\n",
      "Partially Destroyed Housing       0.00      0.00      0.00         2\n",
      "                  Relocated       1.00      0.20      0.33         5\n",
      "                  Sheltered       0.00      0.00      0.00        18\n",
      "\n",
      "                avg / total       0.74      0.67      0.67       312\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report( y_test, combined_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracted Quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted quantity is obtained in two ways:\n",
    "    \n",
    "1. Using the quantity extracted in the most likely report\n",
    "2. Using a second rule based approach that also takes into account the predicted Unit (People / Households) and attempts to find the nearest number to that unit in the text\n",
    "3. If neither of the above approaches yields a result, then returns the largest number found in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the training data, remove non-English excerpts\n",
    "df = pd.read_excel(\"../data/IDMC_fully_labelled.csv.xlsx\", encoding='latin1')\n",
    "df['lang'] = df['Excerpt'].apply(lambda x: interpreter.check_language(x))\n",
    "df = df[df['lang'] == 'en'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[['Excerpt', 'Displacement figure', 'Reporting unit']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1: Existing hand-crafted rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['cleaned_excerpt'] = df['Excerpt'].apply(lambda x: remove_brackets(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['extracted_report'] = df['cleaned_excerpt'].apply(lambda x: get_report(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['extracted_quantity'] = df['extracted_report'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['extracted_quantity'] = df['extracted_quantity'].fillna(0)\n",
    "df['extracted_quantity'] = df['extracted_quantity'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1, support = sklearn.metrics.precision_recall_fscore_support(df['Displacement figure'], df['extracted_quantity'], average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5413223140495868, Recall: 0.5206611570247934, F1: 0.5234159779614325, Support: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: {}, Recall: {}, F1: {}, Support: {}\".format(precision, recall, f1, support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2: Additional set of rules taking into account the Unit (People vs Households)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "person_units = [\"person\", \"people\", \"individuals\", \"locals\", \"villagers\", \"residents\",\n",
    "                \"occupants\", \"citizens\", \"IDP\"]\n",
    "\n",
    "household_units = [\"home\", \"house\", \"hut\", \"dwelling\", \"building\", \"families\", \"households\"]\n",
    "\n",
    "person_lemmas =[t.lemma_ for t in nlp(\" \".join(person_units))]\n",
    "household_lemmas =[t.lemma_ for t in nlp(\" \".join(household_units))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_closest_number(possible_numbers, possible_units):\n",
    "    '''Get closest number in text to word that most closely matches the given unit'''\n",
    "    if len(possible_units) > 0:\n",
    "        first_unit_place = possible_units[0][1]\n",
    "        diffs = [(p, first_unit_place - n) for p, n in possible_numbers if first_unit_place - n > 0]\n",
    "        if len(diffs) > 0:\n",
    "            return sorted(diffs, key=lambda x: x[1])[0][0]\n",
    "    return 0\n",
    "\n",
    "def get_number(text, unit):\n",
    "    '''Get number from text based on the given unit type'''\n",
    "    # Remove brackets\n",
    "    text = re.sub(r'(\\(.+\\))', '', text)\n",
    "    text = re.sub(r'\\s{2}', ' ', text)\n",
    "    text = re.sub(r'(\\d)\\s(\\d)', r'\\1\\2', text)\n",
    "    lemmas = []\n",
    "    if unit == \"People\":\n",
    "        lemmas = person_lemmas\n",
    "    else:\n",
    "        lemmas = household_lemmas\n",
    "    doc = nlp(text)\n",
    "    possible_numbers = []\n",
    "    possible_units = []\n",
    "    for token in doc:\n",
    "        if token.like_num:\n",
    "            quantity = convert_quantity(token.text)\n",
    "            if quantity:\n",
    "                possible_numbers.append((quantity, token.i))\n",
    "        if token.lemma_ in lemmas:\n",
    "            possible_units.append((token, token.i))\n",
    "    \n",
    "    closest_num = get_closest_number(possible_numbers, possible_units)\n",
    "    if closest_num and closest_num > 0:\n",
    "        return closest_num\n",
    "    elif len(possible_numbers) > 0:\n",
    "        highest_possible = sorted(possible_numbers, key=lambda x: x[0])[0][0]\n",
    "        return highest_possible\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['new_extracted_figure'] = df[['Excerpt', 'Reporting unit']].apply(lambda x: get_number(x['Excerpt'], x['Reporting unit']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['new_extracted_figure'] = df['new_extracted_figure'].fillna(0)\n",
    "df['new_extracted_figure'] = df['new_extracted_figure'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1, support = sklearn.metrics.precision_recall_fscore_support(df['Displacement figure'], df['new_extracted_figure'], average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7252066115702479, Recall: 0.7107438016528925, F1: 0.7099567099567099, Support: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: {}, Recall: {}, F1: {}, Support: {}\".format(precision, recall, f1, support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 3: Combine the outputs of approaches 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_quantities(r1, r2):\n",
    "    '''Combine output of reports-based rules & new rules'''\n",
    "    if r1 and r1 > 0:\n",
    "        return r1\n",
    "    elif r2:\n",
    "        return r2\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['combined_quantity'] = df[['extracted_quantity', 'new_extracted_figure']].apply(lambda x: combine_quantities(x['extracted_quantity'], x['new_extracted_figure']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1, support = sklearn.metrics.precision_recall_fscore_support(df['Displacement figure'], df['combined_quantity'], average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7603305785123967, Recall: 0.7603305785123967, F1: 0.7548209366391185, Support: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: {}, Recall: {}, F1: {}, Support: {}\".format(precision, recall, f1, support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Finally, test this combined approach on the NLP test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df['rules_1_quantity'] = test_df['extracted_report'].apply(lambda x: x[0])\n",
    "test_df['rules_2_quantity'] = test_df[['excerpt', 'predicted_unit']].apply(lambda x: get_number(x['excerpt'], x['predicted_unit']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df['combined_quantity'] = test_df[['rules_1_quantity', 'rules_2_quantity']].apply(lambda x: combine_quantities(x['rules_1_quantity'], x['rules_2_quantity']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df['combined_quantity'] = test_df['combined_quantity'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1, support = sklearn.metrics.precision_recall_fscore_support(test_df['Quantity'], test_df['combined_quantity'], average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8279914529914529, Recall: 0.7307692307692307, F1: 0.7614097367773838, Support: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: {}, Recall: {}, F1: {}, Support: {}\".format(precision, recall, f1, support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
