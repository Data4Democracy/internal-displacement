{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a summary of the current 'best' approaches for extracting the relevant terms as requested by IDMC:\n",
    "- Reporting term\n",
    "- Reporting unit\n",
    "- Displacement figure\n",
    "- Location & Country\n",
    "\n",
    "For reference we have:\n",
    "1. Set of ~130 pre-labelled examples provided by IDMC (~118 in English)\n",
    "2. Test set of excerpts that have been hand-labelled\n",
    "\n",
    "Note: in many cases the relevant labels are quite subjective given the vagaries of language; furthermore in some cases multiple 'terms' or 'units' can be mentioned in the same sentence or excerpt; similarly there can often be ambiguous locations within the text\n",
    "\n",
    "In general, in order to choose the most likely or relevant terms from the text, we are using the following heuristics:\n",
    "\n",
    "1. Things affecting 'People' take precedence over things affecting Structures\n",
    "2. Where clarification is given in brackets it is ignored (i.e., 1,000 families (3,000 people) should give 1,000 families)\n",
    "3. Reports of destroyed structures take precedence over reports of damaged structures\n",
    "4. If there is still ambiguity, the first possible report is chosen\n",
    "5. Where multiple possible countries are mentioned:\n",
    "    - The most common country is chosen, or\n",
    "    - The first country is chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting Unit: People or Households\n",
    "\n",
    "Overall Approach: Combine output of rules-based and machine learning model\n",
    "\n",
    "Machine Learning Model: Multinomial Naive Bayes\n",
    "Optimized based on ~130 training examples provided\n",
    "Grid Search for Best Alpha parameter\n",
    "\n",
    "Text Pre-Processing:\n",
    "    \n",
    "1. Remove named entities (People, Dates, Locations, Organizations, Numbers)\n",
    "2. Remove text in brackets\n",
    "3. Clean unusual characters etc.\n",
    "\n",
    "Features:\n",
    "    \n",
    "Words extracted using count vectorizer; stop words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import gensim\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import sklearn.pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from internal_displacement.interpreter import Interpreter\n",
    "from internal_displacement.extracted_report import convert_quantity\n",
    "from internal_displacement.excerpt_helper import Helper\n",
    "from internal_displacement.excerpt_helper import MeanEmbeddingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "person_reporting_terms = [\n",
    "    'displaced', 'evacuated', 'forced', 'flee', 'homeless', 'relief camp',\n",
    "    'sheltered', 'relocated', 'stranded', 'stuck', 'accommodated']\n",
    "\n",
    "structure_reporting_terms = [\n",
    "    'destroyed', 'damaged', 'swept', 'collapsed',\n",
    "    'flooded', 'washed', 'inundated', 'evacuate'\n",
    "]\n",
    "\n",
    "person_reporting_units = [\"families\", \"person\", \"people\", \"individuals\", \"locals\", \"villagers\", \"residents\",\n",
    "                            \"occupants\", \"citizens\", \"households\"]\n",
    "\n",
    "structure_reporting_units = [\"home\", \"house\", \"hut\", \"dwelling\", \"building\"]\n",
    "\n",
    "relevant_article_terms = ['Rainstorm', 'hurricane',\n",
    "                          'tornado', 'rain', 'storm', 'earthquake']\n",
    "relevant_article_lemmas = [t.lemma_ for t in nlp(\n",
    "    \" \".join(relevant_article_terms))]\n",
    "\n",
    "data_path = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interpreter = Interpreter(nlp, person_reporting_terms, structure_reporting_terms, person_reporting_units,\n",
    "                          structure_reporting_units, relevant_article_lemmas, data_path,\n",
    "                          model_path='../internal_displacement/classifiers/default_model.pkl',\n",
    "                          encoder_path='../internal_displacement/classifiers/default_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initializer the helper\n",
    "helper = Helper(nlp, '../internal_displacement/classifiers/unit_vectorizer.pkl', \n",
    "               '../internal_displacement/classifiers/unit_model.pkl',\n",
    "               '../internal_displacement/classifiers/term_vectorizer.pkl',\n",
    "               '../internal_displacement/classifiers/term_model.pkl',\n",
    "               '../internal_displacement/classifiers/terem_svc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the training data, remove non-English excerpts\n",
    "df = pd.read_csv(\"../data/IDMC_fully_labelled.csv\", encoding='latin1')\n",
    "df['lang'] = df['Excerpt'].apply(lambda x: interpreter.check_language(x))\n",
    "df = df[df['lang'] == 'en'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clean excerpts\n",
    "df['cleaned_text'] = df['Excerpt'].apply(lambda x: helper.cleanup(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['reports'] = df['cleaned_text'].apply(lambda x: interpreter.process_article_new(x))\n",
    "df['most_likely_report'] = df['reports'].apply(lambda x: helper.get_report(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use grid-search to optimize for MultiNomial Naive Bayes alpha param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Words as features\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", binary = True, \n",
    "                             ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['Reporting unit'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_features = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [0, 0.1, 1, 10]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = dict(alpha=[0, 0.1, 1, 10])\n",
    "cv = GridSearchCV(clf, param_grid=parameters)\n",
    "cv.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_features = vectorizer.transform(X_test)\n",
    "y_predictions = cv.predict(X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      " Households       1.00      0.50      0.67         6\n",
      "     People       0.86      1.00      0.92        18\n",
      "\n",
      "avg / total       0.89      0.88      0.86        24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report( y_test, y_predictions ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results of only using the rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reports = [interpreter.process_article_new(x) for x in X_test]\n",
    "top_reports = [helper.get_report(x) for x in reports]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_unit = [r[2] for r in top_reports]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "                  0.00      0.00      0.00         0\n",
      " Households       1.00      0.33      0.50         6\n",
      "     People       1.00      0.39      0.56        18\n",
      "\n",
      "avg / total       1.00      0.38      0.55        24\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report( y_test, predicted_unit ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results of using a combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_unit = []\n",
    "\n",
    "for p1, p2 in zip(y_predictions, predicted_unit):\n",
    "    combined_unit.append(helper.combine_predictions(p1, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      " Households       1.00      0.50      0.67         6\n",
      "     People       0.86      1.00      0.92        18\n",
      "\n",
      "avg / total       0.89      0.88      0.86        24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report( y_test, combined_unit ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reporting term can be one of 10 categories (i.e. Displaced, Evacuated, In Relief Camp etc)\n",
    "\n",
    "A similar approach is taken as per reporting unit (i.e. combining outcome of hand-crafted rules & classifier).\n",
    "\n",
    "In this case, the classifier is even harder to train as many of the categories have very few or 0 examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['Reporting term'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "vectorizer_1 = CountVectorizer(analyzer = \"word\", binary = True, \n",
    "                             ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_features = vectorizer_1.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [0, 0.1, 1, 10]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = dict(alpha=[0, 0.1, 1, 10])\n",
    "cv1 = GridSearchCV(clf, param_grid=parameters)\n",
    "cv1.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_features_1 = vectorizer_1.transform(X_test)\n",
    "y_predictions = cv1.predict(X_test_features_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Destroyed Housing       1.00      0.50      0.67         4\n",
      "        Displaced       0.67      0.80      0.73         5\n",
      "        Evacuated       0.67      0.91      0.77        11\n",
      "   Forced to Flee       0.00      0.00      0.00         2\n",
      "        Relocated       1.00      0.50      0.67         2\n",
      "\n",
      "      avg / total       0.69      0.71      0.67        24\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report( y_test, y_predictions ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Linear SVC Using Word2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_2 = MeanEmbeddingVectorizer(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = SVC(probability=True, kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_features = vectorizer_2.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.01, 0.1, 1, 10]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = dict(C=[0.01, 0.1, 1, 10])\n",
    "cv2 = GridSearchCV(clf, param_grid=parameters)\n",
    "cv2.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_features_2 = vectorizer_2.transform(X_test)\n",
    "y_predictions_2 = cv2.predict(X_test_features_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Destroyed Housing       0.67      0.50      0.57         4\n",
      "        Displaced       1.00      0.60      0.75         5\n",
      "        Evacuated       0.61      1.00      0.76        11\n",
      "   Forced to Flee       0.00      0.00      0.00         2\n",
      "        Relocated       0.00      0.00      0.00         2\n",
      "\n",
      "      avg / total       0.60      0.67      0.60        24\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report( y_test, y_predictions_2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the two classifier probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p1 = cv1.predict_proba(X_test_features_1)\n",
    "p2 = cv2.predict_proba(X_test_features_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_combined = helper.combine_probabilities(p1, p2, cv1.best_estimator_.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Destroyed Housing       1.00      0.50      0.67         4\n",
      "        Displaced       0.71      1.00      0.83         5\n",
      "        Evacuated       0.79      1.00      0.88        11\n",
      "   Forced to Flee       0.00      0.00      0.00         2\n",
      "        Relocated       1.00      0.50      0.67         2\n",
      "\n",
      "      avg / total       0.76      0.79      0.74        24\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report( y_test, p_combined ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location & Country Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Country extraction approach is currently based on Spacy entity extraction and hand-crafted rules for translated extracted locations into 3-digit ISO country codes\n",
    "\n",
    "If more than one country is identified in the fragment, the rules for choosing the country are:\n",
    "\n",
    "1. Country that occurs the highest number of times or,\n",
    "2. Country that appears first in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on the provided pre-labelled training data\n",
    "\n",
    "Note: in some cases a country code is still given, even though there are no locations mentioned in the excerpt\n",
    "We ignore these cases for testing purposes (the information likely comes from a different source that will not\n",
    "be included in the specific NLP test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/IDMC_fully_labelled.csv.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Remove those lines where no location is mentioned\n",
    "df = df[df['Mentions'] == 'Y'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_country_code(text):\n",
    "    '''Extract country code from ; separated values'''\n",
    "    return text.split(';')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['true_code'] = df['Location (Country)'].apply(lambda x: get_country_code(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['locations'] = df['Excerpt'].apply(lambda x: interpreter.extract_countries(interpreter.cleanup(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['top_location'] = df['locations'].apply(lambda x: helper.choose_country(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['extracted_code'] = df['top_location'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1, support = sklearn.metrics.precision_recall_fscore_support( df['true_code'], df['extracted_code'], average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8187748783724015, Recall: 0.7105263157894737, F1: 0.7461506329927383, Support: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: {}, Recall: {}, F1: {}, Support: {}\".format(precision, recall, f1, support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracted Quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted quantity is obtained in two ways:\n",
    "    \n",
    "1. Using the quantity extracted in the most likely report\n",
    "2. Using a second rule based approach that also takes into account the predicted Unit (People / Households) and attempts to find the nearest number to that unit in the text\n",
    "3. If neither of the above approaches yields a result, then returns the largest number found in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the training data, remove non-English excerpts\n",
    "df = pd.read_excel(\"../data/IDMC_fully_labelled.csv.xlsx\", encoding='latin1')\n",
    "df['lang'] = df['Excerpt'].apply(lambda x: interpreter.check_language(x))\n",
    "df = df[df['lang'] == 'en'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[['Excerpt', 'Displacement figure', 'Reporting unit']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1: Existing hand-crafted rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['cleaned_excerpt'] = df['Excerpt'].apply(lambda x: remove_brackets(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['extracted_report'] = df['cleaned_excerpt'].apply(lambda x: get_report(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['extracted_quantity'] = df['extracted_report'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['extracted_quantity'] = df['extracted_quantity'].fillna(0)\n",
    "df['extracted_quantity'] = df['extracted_quantity'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1, support = sklearn.metrics.precision_recall_fscore_support(df['Displacement figure'], df['extracted_quantity'], average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5413223140495868, Recall: 0.5206611570247934, F1: 0.5234159779614325, Support: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: {}, Recall: {}, F1: {}, Support: {}\".format(precision, recall, f1, support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2: Additional set of rules taking into account the Unit (People vs Households)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['new_extracted_figure'] = df[['Excerpt', 'Reporting unit']].apply(lambda x: helper.get_number(x['Excerpt'], x['Reporting unit']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['new_extracted_figure'] = df['new_extracted_figure'].fillna(0)\n",
    "df['new_extracted_figure'] = df['new_extracted_figure'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1, support = sklearn.metrics.precision_recall_fscore_support(df['Displacement figure'], df['new_extracted_figure'], average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7252066115702479, Recall: 0.7107438016528925, F1: 0.7099567099567099, Support: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: {}, Recall: {}, F1: {}, Support: {}\".format(precision, recall, f1, support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 3: Combine the outputs of approaches 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['combined_quantity'] = df[['extracted_quantity', 'new_extracted_figure']].apply(lambda x: interpreter.combine_quantities(x['extracted_quantity'], x['new_extracted_figure']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/simonbedford/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1, support = sklearn.metrics.precision_recall_fscore_support(df['Displacement figure'], df['combined_quantity'], average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7603305785123967, Recall: 0.7603305785123967, F1: 0.7548209366391185, Support: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: {}, Recall: {}, F1: {}, Support: {}\".format(precision, recall, f1, support))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
