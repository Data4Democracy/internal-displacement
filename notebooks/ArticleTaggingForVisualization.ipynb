{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Explore ways to tag the different terms identified in each article for later visualization for analysts***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import sys\n",
    "from nltk import Tree\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_newline(text):\n",
    "    ''' Removes new line and &nbsp characters.\n",
    "    '''\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data_extract/article_contents.csv') #Connecting to pre-populated dataset.\n",
    "test_data['content'] = test_data['content'].apply(lambda x: (remove_newline(str(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "person_reporting_terms = [\n",
    "    'displaced', 'evacuated', 'forced flee', 'homeless', 'relief camp',\n",
    "    'sheltered', 'relocated', 'stranded','stuck','stranded',\"killed\",\"dead\",\"died\"\n",
    "]\n",
    "\n",
    "structure_reporting_terms = [\n",
    "    'destroyed','damaged','swept','collapsed','flooded','washed'\n",
    "]\n",
    "\n",
    "person_reporting_units = [\"families\",\"person\",\"people\",\"individuals\",\"locals\",\"villagers\",\"residents\",\"occupants\",\"citizens\", \"households\"]\n",
    "\n",
    "structure_reporting_units = [\"home\",\"house\",\"hut\",\"dwelling\",\"building\",\"shop\",\"business\",\"apartment\",\"flat\",\"residence\"]\n",
    "\n",
    "\n",
    "person_term_lemmas = [t.lemma_ for t in nlp(\" \".join(person_reporting_terms))]\n",
    "structure_term_lemmas = [t.lemma_ for t in nlp(\" \".join(structure_reporting_terms))]\n",
    "person_unit_lemmas = [t.lemma_ for t in nlp(\" \".join(person_reporting_units))]\n",
    "structure_unit_lemmas = [t.lemma_ for t in nlp(\" \".join(structure_reporting_units))]\n",
    "\n",
    "reporting_term_lemmas = person_term_lemmas + structure_term_lemmas\n",
    "reporting_unit_lemmas = person_unit_lemmas + structure_unit_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Report:\n",
    "    def __init__(self,locations,date_time,event_term,subject_term,quantity,story):\n",
    "        self.locations = locations\n",
    "        self.date_time = date_time\n",
    "        self.event_term = [t.lemma_ for t in nlp(event_term)][0]\n",
    "        self.subject_term = subject_term\n",
    "        self.quantity = quantity\n",
    "        self.story = story\n",
    "    \n",
    "    def display(self):\n",
    "        print(\"Location: {}  DateTime: {}  EventTerm: {}  SubjectTerm:  {}  Quantity: {}\"\n",
    "              .format(self.locations,self.date_time,self.event_term,self.subject_term,self.quantity))\n",
    "        \n",
    "    def show_story_tree(self):\n",
    "        self.display()\n",
    "        for sentence in nlp(self.story).sents:\n",
    "            for token in sentence:\n",
    "                if token.lemma_ == self.event_term:\n",
    "                    return to_nltk_tree(sentence.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_token_equality(token_a,token_b):\n",
    "    if token_a.text == token_b.text:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def check_if_collection_contains_token(token,collection):\n",
    "    if any([test_token_equality(token,t) for t in collection]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_if_entity_contains_token(tokens,entity):\n",
    "    \"\"\"\n",
    "    Function to test if a given entity contains at least one of a list of tokens.\n",
    "    param: tokens: A list of tokens\n",
    "    param: entity: A span\n",
    "    \n",
    "    returns: Boolean\n",
    "    \"\"\"\n",
    "    tokens_ = [t.text for t in tokens]\n",
    "    ret = False\n",
    "    for token in entity:\n",
    "        if token.text in tokens_:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_locations(sentence,root=None):\n",
    "    \"\"\"\n",
    "    Examines a sentence and identifies if any of its constituent tokens describe a location.\n",
    "    If a root token is specified, only location tokens below the level of this token in the tree will be examined. \n",
    "    If no root is specified, location tokens will be drawn from the entirety of the span.\n",
    "    param: sentence       a span\n",
    "    param: root           a token\n",
    "    returns: A list of strings, or None\n",
    "    \"\"\"\n",
    "    if not root:\n",
    "        root = sentence.root\n",
    "    descendents = get_descendents(sentence,root)\n",
    "    location_entities = [e for e in nlp(sentence.text).ents if e.label_ == \"GPE\"]\n",
    "    if len(location_entities) > 0:\n",
    "        descendent_location_tokens = []\n",
    "        for location_ent in location_entities:\n",
    "            if check_if_entity_contains_token(location_ent,descendents):\n",
    "                descendent_location_tokens.extend([token for token in location_ent])\n",
    "        contiguous_token_block = get_contiguous_tokens(descendent_location_tokens)\n",
    "\n",
    "        block_locations = match_entities_in_block(location_entities,contiguous_token_block)\n",
    "        return [location.text for location in block_locations]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def extract_dates(sentence,root=None):\n",
    "    \"\"\"\n",
    "    Examines a sentence and identifies if any of its constituent tokens describe a date.\n",
    "    If a root token is specified, only date tokens below the level of this token in the tree will be examined. \n",
    "    If no root is specified, date tokens will be drawn from the entirety of the span.\n",
    "    param: sentence       a span\n",
    "    param: root           a token\n",
    "    returns: A list of strings, or None\n",
    "    \"\"\"\n",
    "    if not root:\n",
    "        root = sentence.root\n",
    "    descendents = get_descendents(sentence,root)\n",
    "    date_entities = [e for e in nlp(sentence.text).ents if e.label_ == \"DATE\"]\n",
    "    if len(date_entities) > 0:\n",
    "        descendent_date_tokens = []\n",
    "        for date_ent in date_entities:\n",
    "            if check_if_entity_contains_token(date_ent,descendents):\n",
    "                descendent_date_tokens.extend([token for token in date_ent])\n",
    "        contiguous_token_block = get_contiguous_tokens(descendent_date_tokens)\n",
    "\n",
    "        block_dates = match_entities_in_block(date_entities,contiguous_token_block)\n",
    "        return [location.text for location in block_dates]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_contiguous_tokens(token_list):\n",
    "    common_ancestor_tokens = get_common_ancestors(token_list)\n",
    "    highest_contiguous_block = []\n",
    "    for toke in token_list:\n",
    "        if check_if_collection_contains_token(toke.head,common_ancestor_tokens):\n",
    "            highest_contiguous_block.append(toke)\n",
    "    added_tokens = 1\n",
    "    while added_tokens > 0:\n",
    "        added_tokens = 0\n",
    "        for toke in token_list:\n",
    "            if check_if_collection_contains_token(toke.head,highest_contiguous_block):\n",
    "                if not check_if_collection_contains_token(toke,highest_contiguous_block):\n",
    "                    highest_contiguous_block.append(toke)\n",
    "                    added_tokens +=1\n",
    "    return highest_contiguous_block\n",
    "\n",
    "def match_entities_in_block(entities,token_block):\n",
    "    matched = []\n",
    "    text_block = [t.text for t in token_block] #For some reason comparing identity on tokens does not always work.\n",
    "    for e in entities:\n",
    "        et = [t.text for t in e]\n",
    "        et_in_b = [t for t in et if t in text_block]\n",
    "        if len(et_in_b) == len(et):\n",
    "            matched.append(e)\n",
    "    return matched\n",
    "\n",
    "def get_common_ancestors(tokens):\n",
    "    ancestors = [set(t.ancestors) for t in tokens]\n",
    "    if len(ancestors) == 0:\n",
    "        return []\n",
    "    common_ancestors = ancestors[0].intersection(*ancestors)\n",
    "    return common_ancestors\n",
    "\n",
    "\n",
    "def get_descendents(sentence,root=None):\n",
    "    \"\"\"\n",
    "    Retrieves all tokens that are descended from the head of the specified root token.\n",
    "    param: root: the root token\n",
    "    param: sentence: a span from which to retrieve tokens.\n",
    "    returns: a list of tokens\n",
    "    \"\"\"\n",
    "    if not root:\n",
    "        root = sentence.root\n",
    "    else:\n",
    "        root = root.head\n",
    "    return [t for t in sentence if root.is_ancestor_of(t)]\n",
    "    \n",
    "def get_all_descendent_tokens(token):\n",
    "    \"\"\"\n",
    "    Returns a list of all descendents of the specified token.\n",
    "    \"\"\"\n",
    "    children_accum = []\n",
    "    for child in token.children:\n",
    "        children_accum.append(child)\n",
    "        grandchildren = get_all_descendent_tokens(child)\n",
    "        children_accum.extend(grandchildren)\n",
    "    return children_accum\n",
    "\n",
    "def process_branch(token):\n",
    "    '''Examines a branch (defined as token and all of its children)\n",
    "    to see if any tokens are number-like and / or reporting units\n",
    "    If a reporting_unit is found, returns the identified unit and any\n",
    "    identified numbers\n",
    "    param: token       a token\n",
    "    return: reporting_unit, number or None, None\n",
    "    '''\n",
    "    children = [token] + get_all_descendent_tokens(token)\n",
    "    reporting_unit, number = None, None\n",
    "    for child in children:\n",
    "        if child.like_num:\n",
    "            number = child.text\n",
    "        elif child.lemma_ in reporting_unit_lemmas:\n",
    "            reporting_unit = child.text\n",
    "    return reporting_unit, number\n",
    "\n",
    "def process_article(story):\n",
    "    '''Process an article by splitting it into sentences and\n",
    "    calling process_sentence for each sentence\n",
    "    Keep a running track of identified dates and locations that\n",
    "    can be used as default values for reports that have no date\n",
    "    or location\n",
    "    param: story       string\n",
    "    return: list of reports\n",
    "    '''\n",
    "    processed_reports = []\n",
    "    sentences = list(nlp(story).sents) # Split into sentences\n",
    "    last_date = None # Keep a running track of the most recent date found in articles\n",
    "    last_location = None # Keep a running track of the most recent location found in articles\n",
    "    for sentence in sentences: # Process sentence\n",
    "        report = process_sentence(sentence, story)\n",
    "        if report:\n",
    "            if report.date_time:\n",
    "                last_date = report.date_time\n",
    "            else:\n",
    "                report.date_time = last_date\n",
    "            if report.locations:\n",
    "                last_location = report.locations\n",
    "            else:\n",
    "                report.locations = last_location\n",
    "            processed_reports.append(report)\n",
    "    return processed_reports\n",
    "\n",
    "def process_sentence(sentence, story):\n",
    "    '''Process a sentence to try and find any reports contained\n",
    "    within it.\n",
    "    First try and find a reporting_term; if it exists identify any\n",
    "    locations and dates.\n",
    "    Finally, look within all branches below the reporting_term to\n",
    "    try and identify a relevant reporting unit and number.\n",
    "    If a minimum of a reporting_term and reporting_unit exist, \n",
    "    then create a report.\n",
    "    param: sentence Spacy sentence\n",
    "    return: report\n",
    "    '''\n",
    "    for token in sentence:\n",
    "        if token.lemma_ in reporting_term_lemmas:\n",
    "            term_token = token\n",
    "            possible_locations = extract_locations(sentence,token)\n",
    "            possible_dates = extract_dates(sentence,token)\n",
    "            reporting_term = term_token.text \n",
    "            children = term_token.children\n",
    "            for child in children:\n",
    "                reporting_unit, number = process_branch(child)\n",
    "                if reporting_unit:\n",
    "                    report = Report(possible_locations,possible_dates,reporting_term,reporting_unit,number,story)\n",
    "                    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Story================\n",
      "Flash flooding across Afghanistan and Pakistan has left more than 160 dead and dozens stranded in one of South Asia's worst natural disasters this year, say officials.  The flooding, caused by unusually heavy rain, has left villagers stuck in remote areas without shelter, food or power.  Mountainous Afghanistan was the worst hit, with 61 people killed and approximately 500 traditional mud-brick homes washed away in more than a dozen villages in Sarobi, a rural district less than an hour from Kabul, officials said.  Floods left a village devastated in the remote eastern Afghan province of Nuristan. At least 60 homes were destroyed across three districts, said provincial spokesman Mohammad Yusufi. No one was killed.  Authorities have been unable to deliver aid to some badly affected villages by land as roads in the area are controlled by the Taliban, Yusufi added.  “We have asked the national government for help as have an overwhelming number of locals asking for assistance, but this is a Taliban-ridden area,” Yusufi said.  At least 24 people were also died in two other eastern border provinces, Khost and Nangarhar, according to local officials. More than fifty homes and shops were destroyed and thousands of acres of farmland flooded.  In Pakistan monsoon rains claimed more than 80 lives, local media reported. Houses collapsing, drowning and electrocution all pushed up the death toll, said Sindh Information Minister Sharjeel Inam Memon.  In Karachi, the commercial capital and a southern port city that is home to 18 million people, poor neighborhoods were submerged waist-deep in water and many precincts suffered long power outages. Deaths were also reported in the north and west of the country.\n",
      "=============Reports================\n",
      "Location: ['Sarobi']  DateTime: None  EventTerm: kill  SubjectTerm:  homes  Quantity: 500\n",
      "Location: ['Sarobi']  DateTime: None  EventTerm: destroy  SubjectTerm:  homes  Quantity: 60\n",
      "Location: ['Khost', 'Nangarhar']  DateTime: None  EventTerm: die  SubjectTerm:  people  Quantity: 24\n",
      "Location: ['Khost', 'Nangarhar']  DateTime: None  EventTerm: destroy  SubjectTerm:  shops  Quantity: fifty\n",
      "Location: ['Khost', 'Nangarhar']  DateTime: None  EventTerm: collapse  SubjectTerm:  Houses  Quantity: None\n"
     ]
    }
   ],
   "source": [
    "article = test_data.iloc[0]['content']\n",
    "print(\"=============Story================\")\n",
    "print(article)\n",
    "print(\"=============Reports================\")\n",
    "reports = process_article(article)\n",
    "for report in reports:\n",
    "    report.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1 - Save a copy of the article, tag complete sentences where something is identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_sentence(sentence):\n",
    "    start_tag = '<mark data-entity=\"report\">'\n",
    "    end_tag = '</mark>'\n",
    "    return start_tag + sentence + end_tag\n",
    "\n",
    "def process_article(story):\n",
    "    processed_reports = []\n",
    "    tagged_article = []\n",
    "    sentences = list(nlp(story).sents) # Split into sentences\n",
    "    last_date = None # Keep a running track of the most recent date found in articles\n",
    "    last_location = None # Keep a running track of the most recent location found in articles\n",
    "    for sentence in sentences: # Process sentence\n",
    "        report = process_sentence(sentence, story)\n",
    "        if report:\n",
    "            tagged_article.append(tag_sentence(sentence.text))\n",
    "            if report.date_time:\n",
    "                last_date = report.date_time\n",
    "            else:\n",
    "                report.date_time = last_date\n",
    "            if report.locations:\n",
    "                last_location = report.locations\n",
    "            else:\n",
    "                report.locations = last_location\n",
    "            processed_reports.append(report)\n",
    "        else:\n",
    "            tagged_article.append(sentence.text)\n",
    "    return processed_reports, tagged_article # If implemented, update Article with tagged version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Tagged Article================\n",
      "[\"Flash flooding across Afghanistan and Pakistan has left more than 160 dead and dozens stranded in one of South Asia's worst natural disasters this year, say officials.  \", 'The flooding, caused by unusually heavy rain, has left villagers stuck in remote areas without shelter, food or power.  ', '<mark data-entity=\"report\">Mountainous Afghanistan was the worst hit, with 61 people killed and approximately 500 traditional mud-brick homes washed away in more than a dozen villages in Sarobi, a rural district less than an hour from Kabul, officials said.  </mark>', 'Floods left a village devastated in the remote eastern Afghan province of Nuristan.', '<mark data-entity=\"report\">At least 60 homes were destroyed across three districts, said provincial spokesman Mohammad Yusufi.</mark>', 'No one was killed.  ', 'Authorities have been unable to deliver aid to some badly affected villages by land as roads in the area are controlled by the Taliban, Yusufi added.  ', '“We have asked the national government for help as have an overwhelming number of locals asking for assistance, but this is a Taliban-ridden area,” Yusufi said.  ', '<mark data-entity=\"report\">At least 24 people were also died in two other eastern border provinces, Khost and Nangarhar, according to local officials.</mark>', '<mark data-entity=\"report\">More than fifty homes and shops were destroyed and thousands of acres of farmland flooded.  </mark>', 'In Pakistan monsoon rains claimed more than 80 lives, local media reported.', '<mark data-entity=\"report\">Houses collapsing, drowning and electrocution all pushed up the death toll, said Sindh Information Minister Sharjeel Inam Memon.  </mark>', 'In Karachi, the commercial capital and a southern port city that is home to 18 million people, poor neighborhoods were submerged waist-deep in water and many precincts suffered long power outages.', 'Deaths were also reported in the north and west of the country.']\n"
     ]
    }
   ],
   "source": [
    "article = test_data.iloc[0]['content']\n",
    "print(\"=============Tagged Article================\")\n",
    "reports, tagged_article = process_article(article)\n",
    "print(tagged_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2 - Save a copy of the article, tag individual tokens where something is identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This currently ignores the indices for dates and locations, which would need to be resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_token(token, data_type):\n",
    "    start_tag = '<mark data-entity=\"{}\">'.format(data_type)\n",
    "    end_tag = '</mark>'\n",
    "    return start_tag + token + end_tag\n",
    "\n",
    "def apply_tags(story, tag_set):\n",
    "    if tag_set['reporting_term']:\n",
    "        story[tag_set['reporting_term']] = tag_token(story[tag_set['reporting_term']], 'reporting_term')\n",
    "    if tag_set['reporting_unit']:\n",
    "        story[tag_set['reporting_unit']] = tag_token(story[tag_set['reporting_unit']], 'reporting_unit')\n",
    "    if tag_set['number']:\n",
    "        story[tag_set['number']] = tag_token(story[tag_set['number']], 'number')\n",
    "    if tag_set['dates']:\n",
    "        for idx in tag_set['dates']:\n",
    "            story[idx] = tag_token(story[idx], 'date')\n",
    "    if tag_set['locations']:\n",
    "        for idx in tag_set['locations']:\n",
    "            story[idx] = tag_token(story[idx], 'location')\n",
    "    return story\n",
    "\n",
    "def apply_tags_to_article(article, tag_indices):\n",
    "    if len(tag_indices) > 0:\n",
    "        story = [tag.text for tag in article]\n",
    "        for tag_set in tag_indices:\n",
    "            story = apply_tags(story, tag_set)\n",
    "    return \" \".join(story) + \".\"\n",
    "    \n",
    "def extract_locations(sentence,root=None):\n",
    "    if not root:\n",
    "        root = sentence.root\n",
    "    descendents = get_descendents(sentence,root)\n",
    "    location_entities = [e for e in nlp(sentence.text).ents if e.label_ == \"GPE\"]\n",
    "    if len(location_entities) > 0:\n",
    "        descendent_location_tokens = []\n",
    "        for location_ent in location_entities:\n",
    "            if check_if_entity_contains_token(location_ent,descendents):\n",
    "                descendent_location_tokens.extend([token for token in location_ent])\n",
    "        contiguous_token_block = get_contiguous_tokens(descendent_location_tokens)\n",
    "\n",
    "        block_locations = match_entities_in_block(location_entities,contiguous_token_block)\n",
    "        return [location.text for location in block_locations], None\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "def extract_dates(sentence,root=None):\n",
    "    if not root:\n",
    "        root = sentence.root\n",
    "    descendents = get_descendents(sentence,root)\n",
    "    date_entities = [e for e in nlp(sentence.text).ents if e.label_ == \"DATE\"]\n",
    "    if len(date_entities) > 0:\n",
    "        descendent_date_tokens = []\n",
    "        for date_ent in date_entities:\n",
    "            if check_if_entity_contains_token(date_ent,descendents):\n",
    "                descendent_date_tokens.extend([token for token in date_ent])\n",
    "        contiguous_token_block = get_contiguous_tokens(descendent_date_tokens)\n",
    "\n",
    "        block_dates = match_entities_in_block(date_entities,contiguous_token_block)\n",
    "        return [location.text for location in block_dates], None\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def process_branch(token):\n",
    "    children = [token] + get_all_descendent_tokens(token)\n",
    "    reporting_unit, number = None, None\n",
    "    reporting_unit_idx, number_idx = None, None\n",
    "    for child in children:\n",
    "        if child.like_num:\n",
    "            number = child.text\n",
    "            number_idx = child.i\n",
    "        elif child.lemma_ in reporting_unit_lemmas:\n",
    "            reporting_unit = child.text\n",
    "            reporting_unit_idx = child.i\n",
    "    return reporting_unit, number, (reporting_unit_idx, number_idx)\n",
    "\n",
    "def process_sentence(sentence, story):\n",
    "    tag_indices = {\n",
    "        'dates': None, 'locations': None, 'reporting_term': None,\n",
    "        'reporting_unit': None, 'number': None }\n",
    "    for token in sentence:\n",
    "        if token.lemma_ in reporting_term_lemmas:\n",
    "            tag_indices['reporting_term'] = token.i\n",
    "            term_token = token\n",
    "            possible_locations, locations_indices = extract_locations(sentence,token)\n",
    "            tag_indices['locations'] = locations_indices\n",
    "            possible_dates, dates_indices = extract_dates(sentence,token)\n",
    "            tag_indices['dates'] = dates_indices\n",
    "            reporting_term = term_token.text \n",
    "            children = term_token.children\n",
    "            for child in children:\n",
    "                reporting_unit, number, indices = process_branch(child)\n",
    "                if reporting_unit:\n",
    "                    tag_indices['reporting_unit'] = indices[0]\n",
    "                    tag_indices['number'] = indices[1]\n",
    "                    report = Report(possible_locations,possible_dates,reporting_term,reporting_unit,number,story)\n",
    "                    return report, tag_indices\n",
    "    return None, None\n",
    "                \n",
    "def process_article(story):\n",
    "    processed_reports = []\n",
    "    article_report_indices = []\n",
    "    story = nlp(story)\n",
    "    sentences = list(story.sents) # Split into sentences\n",
    "    last_date = None # Keep a running track of the most recent date found in articles\n",
    "    last_location = None # Keep a running track of the most recent location found in articles\n",
    "    for sentence in sentences: # Process sentence\n",
    "        report, report_indices = process_sentence(sentence, story)\n",
    "        if report:\n",
    "            article_report_indices.append(report_indices)\n",
    "            if report.date_time:\n",
    "                last_date = report.date_time\n",
    "            else:\n",
    "                report.date_time = last_date\n",
    "            if report.locations:\n",
    "                last_location = report.locations\n",
    "            else:\n",
    "                report.locations = last_location\n",
    "            processed_reports.append(report)\n",
    "    tagged_article = apply_tags_to_article(story, article_report_indices)\n",
    "    return processed_reports, tagged_article # If implemented, update Article with tagged version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Tagged Article================\n",
      "Flash flooding across Afghanistan and Pakistan has left more than 160 dead and dozens stranded in one of South Asia 's worst natural disasters this year , say officials .   The flooding , caused by unusually heavy rain , has left villagers stuck in remote areas without shelter , food or power .   Mountainous Afghanistan was the worst hit , with 61 people <mark data-entity=\"reporting_term\">killed</mark> and approximately <mark data-entity=\"number\">500</mark> traditional mud - brick <mark data-entity=\"reporting_unit\">homes</mark> washed away in more than a dozen villages in Sarobi , a rural district less than an hour from Kabul , officials said .   Floods left a village devastated in the remote eastern Afghan province of Nuristan . At least <mark data-entity=\"number\">60</mark> <mark data-entity=\"reporting_unit\">homes</mark> were <mark data-entity=\"reporting_term\">destroyed</mark> across three districts , said provincial spokesman Mohammad Yusufi . No one was killed .   Authorities have been unable to deliver aid to some badly affected villages by land as roads in the area are controlled by the Taliban , Yusufi added .   “ We have asked the national government for help as have an overwhelming number of locals asking for assistance , but this is a Taliban - ridden area , ” Yusufi said .   At least <mark data-entity=\"number\">24</mark> <mark data-entity=\"reporting_unit\">people</mark> were also <mark data-entity=\"reporting_term\">died</mark> in two other eastern border provinces , Khost and Nangarhar , according to local officials . More than <mark data-entity=\"number\">fifty</mark> homes and <mark data-entity=\"reporting_unit\">shops</mark> were <mark data-entity=\"reporting_term\">destroyed</mark> and thousands of acres of farmland flooded .   In Pakistan monsoon rains claimed more than 80 lives , local media reported . <mark data-entity=\"reporting_unit\">Houses</mark> <mark data-entity=\"reporting_term\">collapsing</mark> , drowning and electrocution all pushed up the death toll , said Sindh Information Minister Sharjeel Inam Memon .   In Karachi , the commercial capital and a southern port city that is home to 18 million people , poor neighborhoods were submerged waist - deep in water and many precincts suffered long power outages . Deaths were also reported in the north and west of the country ..\n"
     ]
    }
   ],
   "source": [
    "article = test_data.iloc[0]['content']\n",
    "print(\"=============Tagged Article================\")\n",
    "reports, tagged_article = process_article(article)\n",
    "print(tagged_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do: Option 3: \n",
    "\n",
    "For each report save the spans for the extracted entities with start index, end index and type, i.e:\n",
    "\n",
    "spans = [ { end: 20, start: 5, type: \"PERSON\" }, { end: 67, start: 61, type: \"ORG\" }, { end: 75, start: 71, type: \"DATE\" } ];\n",
    "\n",
    "These can then be displayed along with the article using:\n",
    "\n",
    "displacy.render(text, spans, ents)\n",
    "\n",
    "from https://github.com/explosion/displacy-ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
