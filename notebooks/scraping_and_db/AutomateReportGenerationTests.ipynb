{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Start trying to implement some slightly more automaated tests in order to identify areas for improvement and ensure existing functionality is not 'broken' as enhancements are made **\n",
    "\n",
    "This is based upon the following ideas:\n",
    "\n",
    "1. Hand-crafted set of articles and associated reports expected to be generated for each article\n",
    "2. Test function runs `process_article_new` on the test set and compares the generated articles to the expected articles\n",
    "3. Reports are compared by hashing their string representations\n",
    "4. Sets of reports are considered equal if they are of the same length, and the report hashes are equal\n",
    "5. Prints out missing reports (ie. those in expected but not in generated), and erroneous reports (i.e. those in generated but not in expected)\n",
    "\n",
    "Once agreement is reached on what the expected reports should look like, this test dataset could be saved to file for use in testing.\n",
    "\n",
    "In many cases errors are due to discrepancies in specific report details i.e., dates, locations, quantities etc.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import sys\n",
    "from nltk import Tree\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from internal_displacement.pipeline import SQLArticleInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import textacy\n",
    "import re\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = SQLArticleInterface(\"../sql_db.sqlite\") #Connecting to pre-populated database.\n",
    "labels,features = pipeline.get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "person_reporting_terms = [\n",
    "    'displaced', 'evacuated', 'forced','flee', 'homeless', 'relief camp',\n",
    "    'sheltered', 'relocated', 'stranded','stuck','stranded',\"killed\",\"dead\",\"died\",\"drown\"\n",
    "]\n",
    "\n",
    "structure_reporting_terms = [\n",
    "    'destroyed','damaged','swept','collapsed','flooded','washed', 'inundated', 'evacuate'\n",
    "]\n",
    "\n",
    "person_reporting_units = [\"families\",\"person\",\"people\",\"individuals\",\"locals\",\"villagers\",\"residents\",\"occupants\",\"citizens\", \"households\"]\n",
    "\n",
    "structure_reporting_units = [\"home\",\"house\",\"hut\",\"dwelling\",\"building\",\"shop\",\"business\",\"apartment\",\"flat\",\"residence\"]\n",
    "\n",
    "\n",
    "person_term_lemmas = [t.lemma_ for t in nlp(\" \".join(person_reporting_terms))]\n",
    "structure_term_lemmas = [t.lemma_ for t in nlp(\" \".join(structure_reporting_terms))]\n",
    "person_unit_lemmas = [t.lemma_ for t in nlp(\" \".join(person_reporting_units))]\n",
    "structure_unit_lemmas = [t.lemma_ for t in nlp(\" \".join(structure_reporting_units))]\n",
    "\n",
    "reporting_term_lemmas = person_term_lemmas + structure_term_lemmas\n",
    "reporting_unit_lemmas = person_unit_lemmas + structure_unit_lemmas\n",
    "\n",
    "relevant_article_terms = ['Rainstorm', 'hurricane', 'tornado', 'rain', 'storm', 'earthquake']\n",
    "relevant_article_lemmas = [t.lemma_ for t in nlp(\" \".join(relevant_article_terms))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Report:\n",
    "    def __init__(self,locations,date_times,event_term,subject_term,quantity,story):\n",
    "        self.locations = locations\n",
    "        if date_times:\n",
    "            self.date_times = date_times\n",
    "        else:\n",
    "            self.date_times = []\n",
    "        self.event_term = event_term\n",
    "        self.subject_term = subject_term\n",
    "        self.quantity = quantity\n",
    "        self.story = story\n",
    "    \n",
    "    def display(self):\n",
    "        print(\"Location: {}  DateTime: {}  EventTerm: {}  SubjectTerm:  {}  Quantity: {}\"\n",
    "              .format(self.locations,self.date_times,self.event_term,self.subject_term,self.quantity))\n",
    "        \n",
    "    def show_story_tree(self):\n",
    "        self.display()\n",
    "        for sentence in nlp(self.story).sents:\n",
    "            for token in sentence:\n",
    "                if token.lemma_ == self.event_term:\n",
    "                    return to_nltk_tree(sentence.root)\n",
    "                \n",
    "    def report_hash(self):\n",
    "        report_string = \"Location: {}  DateTime: {}  EventTerm: {}  SubjectTerm:  {}  Quantity: {}\".format(self.locations,self.date_times,self.event_term,self.subject_term,self.quantity)\n",
    "        hash1 = hashlib.md5(report_string.encode('utf-8')).hexdigest()\n",
    "        return hash1\n",
    "    \n",
    "    def to_json(self):\n",
    "        d = {}\n",
    "        d['Location'] = self.locations\n",
    "        d['DateTime'] = self.date_times\n",
    "        d['EventTerm'] = self.event_term\n",
    "        d['SubjectTerm'] = self.subject_term\n",
    "        d['Quantity'] = self.quantity\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_token_equality(token_a,token_b):\n",
    "    if token_a.i == token_b.i:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def check_if_collection_contains_token(token,collection):\n",
    "    for c in collection:\n",
    "        if test_token_equality(token,c):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_descendents(sentence,root=None):\n",
    "    \"\"\"\n",
    "    Retrieves all tokens that are descended from the specified root token.\n",
    "    param: root: the root token\n",
    "    param: sentence: a span from which to retrieve tokens.\n",
    "    returns: a list of tokens\n",
    "    \"\"\"\n",
    "    if not root:\n",
    "        root = sentence.root\n",
    "    return [t for t in sentence if root.is_ancestor_of(t)]\n",
    "\n",
    "def get_head_descendents(sentence,root=None):\n",
    "    \"\"\"\n",
    "    Retrieves all tokens that are descended from the head of the specified root token.\n",
    "    param: root: the root token\n",
    "    param: sentence: a span from which to retrieve tokens.\n",
    "    returns: a list of tokens\n",
    "    \"\"\"\n",
    "    if not root:\n",
    "        root = sentence.root\n",
    "    else:\n",
    "        root = root.head\n",
    "    return [t for t in sentence if root.is_ancestor_of(t)]\n",
    "    \n",
    "def check_if_entity_contains_token(tokens,entity):\n",
    "    \"\"\"\n",
    "    Function to test if a given entity contains at least one of a list of tokens.\n",
    "    param: tokens: A list of tokens\n",
    "    param: entity: A span\n",
    "    \n",
    "    returns: Boolean\n",
    "    \"\"\"\n",
    "    tokens_ = [t.text for t in tokens]\n",
    "    ret = False\n",
    "    for token in entity:\n",
    "        if token.text in tokens_:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "def get_distance_from_root(token,root):\n",
    "    \"\"\"\n",
    "    Gets the parse tree distance between a token and the sentence root.\n",
    "    :param token: a token\n",
    "    :param root: the root token of the sentence\n",
    "    \n",
    "    returns: an integer distance\n",
    "    \"\"\"\n",
    "    if token == root:\n",
    "        return 0\n",
    "    d = 1\n",
    "    p = token.head\n",
    "    while p is not root:\n",
    "        d+=1\n",
    "        p = p.head\n",
    "    return d\n",
    "\n",
    "\n",
    "def get_common_ancestors(tokens):\n",
    "    ancestors = [set(t.ancestors) for t in tokens]\n",
    "    if len(ancestors) == 0:\n",
    "        return []\n",
    "    common_ancestors = ancestors[0].intersection(*ancestors)\n",
    "    return common_ancestors    \n",
    "\n",
    "\n",
    "def get_distance_between_tokens(token_a,token_b):\n",
    "\n",
    "    if token_b in token_a.subtree:\n",
    "        distance = get_distance_from_root(token_b,token_a)\n",
    "    elif token_a in token_b.subtree:\n",
    "        distance = get_distance_from_root(token_a,token_b)\n",
    "    else:\n",
    "        common_ancestors = get_common_ancestors([token_a,token_b])\n",
    "        distance = 10000\n",
    "        for ca in common_ancestors:\n",
    "            distance_a = get_distance_from_root(ca,token_a)\n",
    "            distance_b = get_distance_from_root(ca,token_b)\n",
    "            distance_ab = distance_a + distance_b\n",
    "            if distance_ab < distance:\n",
    "                distance = distance_ab\n",
    "    return distance\n",
    "\n",
    "\n",
    "def get_closest_contiguous_location_block(entity_list,root_node):\n",
    "    location_entity_tokens = [[token for token in sentence] for sentence in entity_list]\n",
    "    token_list =  [item for sublist in location_entity_tokens for item in sublist]\n",
    "    location_tokens_by_distance = sorted([(token,get_distance_between_tokens(token,root_node)) \n",
    "                                          for token in token_list],key= lambda x: x[1])\n",
    "    closest_location = location_tokens_by_distance[0]\n",
    "    contiguous_block = [closest_token]\n",
    "    added_tokens = 1\n",
    "    while added_tokens > 0:\n",
    "        contiguous_block_ancestors = [[token for token in token_list if token.is_ancestor_of(toke)] for toke in contiguous_block ]\n",
    "        contiguous_block_subtrees = [token.subtree for token in contiguous_block]\n",
    "        contiguous_block_neighbours = contiguous_block_ancestors + contiguous_block_subtrees\n",
    "        contiguous_block_neighbours = [item for sublist in contiguous_block_neighbours for item in sublist]\n",
    "        added_tokens = 0\n",
    "        for toke in token_list:\n",
    "            if not check_if_collection_contains_token(toke,contiguous_block):\n",
    "                if toke in contiguous_block_neighbours:\n",
    "                    added_tokens +=1\n",
    "                    contiguous_block.append(toke)\n",
    "    return contiguous_block\n",
    "\n",
    "\n",
    "\n",
    "def get_contiguous_tokens(token_list):\n",
    "    common_ancestor_tokens = get_common_ancestors(token_list)\n",
    "    highest_contiguous_block = []\n",
    "    for toke in token_list:\n",
    "        if check_if_collection_contains_token(toke.head,common_ancestor_tokens):\n",
    "            highest_contiguous_block.append(toke)\n",
    "    added_tokens = 1\n",
    "    while added_tokens > 0:\n",
    "        added_tokens = 0\n",
    "        for toke in token_list:\n",
    "            if check_if_collection_contains_token(toke.head,highest_contiguous_block):\n",
    "                if not check_if_collection_contains_token(toke,highest_contiguous_block):\n",
    "                    highest_contiguous_block.append(toke)\n",
    "                    added_tokens +=1\n",
    "    return highest_contiguous_block\n",
    "\n",
    "def match_entities_in_block(entities,token_block):\n",
    "    matched = []\n",
    "    text_block = [t.text for t in token_block] #For some reason comparing identity on tokens does not always work.\n",
    "    for e in entities:\n",
    "        et = [t.text for t in e]\n",
    "        et_in_b = [t for t in et if t in text_block]\n",
    "        if len(et_in_b) == len(et):\n",
    "            matched.append(e)\n",
    "    return matched\n",
    "\n",
    "def extract_locations(sentence,root=None):\n",
    "    \"\"\"\n",
    "    Examines a sentence and identifies if any of its constituent tokens describe a location.\n",
    "    If a root token is specified, only location tokens below the level of this token in the tree will be examined. \n",
    "    If no root is specified, location tokens will be drawn from the entirety of the span.\n",
    "    param: sentence       a span\n",
    "    param: root           a token\n",
    "    returns: A list of strings, or None\n",
    "    \"\"\"\n",
    "\n",
    "    if not root:\n",
    "        root = sentence.root\n",
    "    descendents = get_descendents(sentence,root)\n",
    "    location_entities = [e for e in nlp(sentence.text).ents if e.label_ == \"GPE\"]\n",
    "    if len(location_entities) > 0:\n",
    "        descendent_location_tokens = []\n",
    "        for location_ent in location_entities:\n",
    "            if check_if_entity_contains_token(location_ent,descendents):\n",
    "                descendent_location_tokens.extend([token for token in location_ent])\n",
    "        contiguous_token_block = get_contiguous_tokens(descendent_location_tokens)\n",
    "\n",
    "        block_locations = match_entities_in_block(location_entities,contiguous_token_block)\n",
    "        if len(block_locations) > 0:\n",
    "            return [location.text for location in block_locations]\n",
    "        else:\n",
    "            return [location.text for location in location_entities] #If we cannot decide which one is correct, choose them all\n",
    "                                    #and figure it out at the report merging stage.\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "    \n",
    "    \n",
    "def extract_dates(sentence,root=None):\n",
    "    \"\"\"\n",
    "    Examines a sentence and identifies if any of its constituent tokens describe a date.\n",
    "    If a root token is specified, only date tokens below the level of this token in the tree will be examined. \n",
    "    If no root is specified, date tokens will be drawn from the entirety of the span.\n",
    "    Unlike the extract dates function (which returns a list of strings),\n",
    "    this function returns a list of spacy spans. This is because numerical quantities detected in the \n",
    "    branch_search need to be checked to ensure they are not in fact parts of a date.\n",
    "    \n",
    "    param: sentence       a span\n",
    "    param: root           a token\n",
    "    returns: A list of spacy spans\n",
    "    \"\"\"\n",
    "    if not root:\n",
    "        root = sentence.root\n",
    "    descendents = get_head_descendents(sentence,root)\n",
    "    date_entities = [e for e in nlp(sentence.text).ents if e.label_ == \"DATE\"]\n",
    "    if len(date_entities) > 0:\n",
    "        descendent_date_tokens = []\n",
    "        for date_ent in date_entities:\n",
    "            if check_if_entity_contains_token(date_ent,descendents):\n",
    "                descendent_date_tokens.extend([token for token in date_ent])\n",
    "        contiguous_token_block = get_contiguous_tokens(descendent_date_tokens)\n",
    "\n",
    "        block_dates = match_entities_in_block(date_entities,contiguous_token_block)\n",
    "        return [date.text for date in block_dates]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def basic_number(token):\n",
    "    if token.text in (\"dozens\", \"hundreds\", \"thousands\"):\n",
    "        return True\n",
    "    if token.like_num:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_sentence_new(sentence, dates_memory, locations_memory, story):\n",
    "    \"\"\"\n",
    "    Extracts the main verbs from a sentence as a starting point\n",
    "    for report extraction.\n",
    "    \"\"\"\n",
    "    sentence_reports = []\n",
    "    # Find the verbs\n",
    "    main_verbs = textacy.spacy_utils.get_main_verbs_of_sent(sentence)\n",
    "    for v in main_verbs:\n",
    "        unit_type, verb_lemma = verb_relevance(v, story)\n",
    "        if unit_type:\n",
    "            reports = branch_search_new(v, verb_lemma, unit_type, dates_memory, locations_memory, sentence, story)\n",
    "            sentence_reports.extend(reports)\n",
    "    return sentence_reports\n",
    "\n",
    "def article_relevance(article):\n",
    "    for token in article:\n",
    "        if token.lemma_ in relevant_article_lemmas:\n",
    "            return True\n",
    "\n",
    "def verb_relevance(verb, article):\n",
    "    \"\"\"\n",
    "    Checks a verb for relevance by:\n",
    "    1. Comparing to structure term lemmas\n",
    "    2. Comparing to person term lemmas\n",
    "    3. Looking for special cases such as 'leave homeless'\n",
    "    \"\"\"\n",
    "    if verb.lemma_ in structure_term_lemmas:\n",
    "        return structure_unit_lemmas, verb.lemma_\n",
    "    elif verb.lemma_ in person_term_lemmas:\n",
    "        return person_unit_lemmas, verb.lemma_\n",
    "    elif verb.lemma_ == 'leave':\n",
    "        children = verb.children\n",
    "        obj_predicate = None\n",
    "        for child in children:\n",
    "            if child.dep_ in ('oprd', 'dobj'):\n",
    "                obj_predicate = child\n",
    "        if obj_predicate:\n",
    "            if obj_predicate.lemma_ in structure_term_lemmas:\n",
    "                return structure_unit_lemmas, 'leave ' + obj_predicate.lemma_\n",
    "            elif obj_predicate.lemma_ in person_term_lemmas:\n",
    "                return person_unit_lemmas, 'leave ' + obj_predicate.lemma_\n",
    "    elif verb.lemma_ == 'affect' and article_relevance(article):\n",
    "        return structure_unit_lemmas + person_unit_lemmas , verb.lemma_\n",
    "    elif verb.lemma_ in ('fear', 'assume'):\n",
    "        verb_objects = textacy.spacy_utils.get_objects_of_verb(verb)\n",
    "        if verb_objects:\n",
    "            verb_object = verb_objects[0]\n",
    "            if verb_object.lemma_ in person_term_lemmas:\n",
    "                return person_unit_lemmas, verb.lemma_ + \" \" + verb_object.text\n",
    "            elif verb_object.lemma_ in structure_term_lemmas:\n",
    "                return structure_unit_lemmas, verb.lemma_ + \" \" + verb_object.text\n",
    "        \n",
    "    return None, None\n",
    "\n",
    "def get_quantity_from_phrase(phrase):\n",
    "    \"\"\"\n",
    "    Look for number-like tokens within noun phrase.\n",
    "    \"\"\"\n",
    "    for token in phrase:\n",
    "        if basic_number(token):\n",
    "            return token\n",
    "            \n",
    "def get_quantity(sentence, unit):\n",
    "    \"\"\"\n",
    "    Split a sentence into noun phrases.\n",
    "    Search for quantities within each noun phrase.\n",
    "    If the noun phrase is part of a conjunction, then\n",
    "    search for quantity within preceding noun phrase\n",
    "    \"\"\"\n",
    "    noun_phrases = list(nlp(sentence.text).noun_chunks)\n",
    "    # Case one - see if phrase contains the unit\n",
    "    for i, np in enumerate(noun_phrases):\n",
    "        if check_if_collection_contains_token(unit,np):\n",
    "            if unit.dep_ == 'conj':\n",
    "                return get_quantity_from_phrase(noun_phrases[i-1])\n",
    "            else:\n",
    "                return get_quantity_from_phrase(np)\n",
    "    #Case two - get any numeric child of the unit noun.\n",
    "    for child in unit.children:\n",
    "        if basic_number(child):\n",
    "            return child\n",
    "    \n",
    "\n",
    "def simple_subjects_and_objects(verb):\n",
    "    verb_objects = textacy.spacy_utils.get_objects_of_verb(verb)\n",
    "    verb_subjects = textacy.spacy_utils.get_subjects_of_verb(verb)\n",
    "    verb_objects.extend(verb_subjects)\n",
    "    return verb_objects\n",
    "\n",
    "\n",
    "def nouns_from_relative_clause(sentence, verb):\n",
    "    possible_clauses = list(textacy.extract.pos_regex_matches(sentence, r'<NOUN>+<VERB>'))\n",
    "    for clause in possible_clauses:\n",
    "        if verb in clause:\n",
    "            for token in clause:\n",
    "                if token.tag_ == 'NNS':\n",
    "                    return token\n",
    "\n",
    "            \n",
    "def get_subjects_and_objects(story, sentence, verb):\n",
    "    \"\"\"\n",
    "    Identify subjects and objects for a verb\n",
    "    Also check if a reporting unit directly precedes\n",
    "    a verb and is a direct or prepositional object\n",
    "    \"\"\"\n",
    "    # Get simple or standard subjects and objects\n",
    "    verb_objects = simple_subjects_and_objects(verb)\n",
    "    # Special Cases\n",
    "\n",
    "    #see if unit directly precedes verb\n",
    "    if verb.i > 0:\n",
    "        preceding = story[verb.i - 1]\n",
    "        if preceding.dep_ in ('pobj', 'dobj') and preceding not in verb_objects:\n",
    "            verb_objects.append(preceding)\n",
    "\n",
    "    # See if verb is part of a conjunction\n",
    "    if verb.dep_ == 'conj':\n",
    "        lefts = list(verb.lefts)\n",
    "        if len(lefts) > 0:\n",
    "            for token in lefts:\n",
    "                if token.dep_ in ('nsubj', 'nsubjpass'):\n",
    "                    verb_objects.append(token)\n",
    "        else:            \n",
    "            ancestors = verb.ancestors\n",
    "            for anc in ancestors:\n",
    "                verb_objects.extend(simple_subjects_and_objects(anc))\n",
    "            \n",
    "    # Look for 'pobj' in sentence\n",
    "    if verb.dep_ == 'ROOT':\n",
    "        for token in sentence:\n",
    "            if token.dep_ == 'pobj':\n",
    "                verb_objects.append(token)\n",
    "                \n",
    "    # Look for nouns in relative clauses\n",
    "    if verb.dep_ == 'relcl':\n",
    "        relcl_noun = nouns_from_relative_clause(sentence, verb)\n",
    "        if relcl_noun:\n",
    "            verb_objects.append(relcl_noun)\n",
    "        \n",
    "    \n",
    "    return list(set(verb_objects))\n",
    "\n",
    "\n",
    "def test_noun_conj(sentence, noun):\n",
    "    possible_conjs = list(textacy.extract.pos_regex_matches(sentence, r'<NOUN><CONJ><NOUN>'))\n",
    "    for conj in possible_conjs:\n",
    "        if noun in conj:\n",
    "            return conj\n",
    "\n",
    "            \n",
    "def branch_search_new(verb, verb_lemma, search_type, dates_memory, locations_memory, sentence, story):\n",
    "    \"\"\"\n",
    "    Extract reports based upon an identified verb (reporting term).\n",
    "    Extract possible locations or use most recent locations\n",
    "    Extract possible dates or use most recent dates\n",
    "    Identify reporting unit by looking in objects and subjects of reporting term (verb)\n",
    "    Identify quantity by looking in noun phrases.\n",
    "    \"\"\"\n",
    "    possible_locations = extract_locations(sentence,verb)\n",
    "    possible_dates = extract_dates(sentence)\n",
    "    if not possible_locations:\n",
    "        possible_locations = locations_memory\n",
    "    if not possible_dates:\n",
    "        possible_dates = dates_memory\n",
    "    reports = []\n",
    "    quantity = None\n",
    "    verb_objects = get_subjects_and_objects(story, sentence, verb)\n",
    "    #If there are multiple possible nouns and it is unclear which is the correct one\n",
    "    #choose the one with the fewest descendents. A verb object with many descendents is more likely to \n",
    "    #have its own verb as a descendent.\n",
    "    verb_descendent_counts = [(v,len(list(v.subtree))) for v in verb_objects]\n",
    "    verb_objects = [x[0] for x in sorted(verb_descendent_counts,key = lambda x: x[1])]\n",
    "    for o in verb_objects:\n",
    "        if basic_number(o) and o.i == (verb.i - 1):\n",
    "            quantity = o\n",
    "            if search_type == structure_term_lemmas:\n",
    "                unit = 'house'\n",
    "            else:\n",
    "                unit = 'person'\n",
    "            report = Report(possible_locations, possible_dates, verb_lemma,\n",
    "                                    unit, quantity, story.text)\n",
    "            #report.display()\n",
    "            reports.append(report)\n",
    "            break\n",
    "        elif o.lemma_ in search_type:\n",
    "            reporting_unit = o.lemma_\n",
    "            noun_conj = test_noun_conj(sentence, o)\n",
    "            if noun_conj:\n",
    "                reporting_unit = noun_conj\n",
    "            # Try and get a number\n",
    "            quantity = get_quantity(sentence, o)\n",
    "            report = Report(possible_locations, possible_dates, verb_lemma,\n",
    "                                    reporting_unit, quantity, story.text)\n",
    "            reports.append(report)\n",
    "            #report.display()\n",
    "            break\n",
    "    return reports\n",
    "\n",
    "def cleanup(text):\n",
    "    text = re.sub(r'([a-zA-Z0-9])(IMPACT)', r'\\1. \\2', text)\n",
    "    text = re.sub(r'([a-zA-Z0-9])(RESPONSE)', r'\\1. \\2', text)\n",
    "    text = re.sub(r'(IMPACT)([a-zA-Z0-9])', r'\\1. \\2', text)\n",
    "    text = re.sub(r'(RESPONSE)([a-zA-Z0-9])', r'\\1. \\2', text)\n",
    "    text = re.sub(r'([a-zA-Z])(\\d)', r'\\1. \\2', text)\n",
    "    return text\n",
    "\n",
    "def process_article_new(story):\n",
    "    \"\"\"\n",
    "    Process a story once sentence at a time\n",
    "    \"\"\"\n",
    "    story = cleanup(story)\n",
    "    processed_reports = []\n",
    "    story = nlp(story)\n",
    "    sentences = list(story.sents) # Split into sentences\n",
    "    dates_memory = None # Keep a running track of the most recent dates found in articles\n",
    "    locations_memory = None # Keep a running track of the most recent locations found in articles\n",
    "    for sentence in sentences: # Process sentence\n",
    "        reports = []\n",
    "        reports = process_sentence_new(sentence, dates_memory, locations_memory, story)\n",
    "        current_locations = extract_locations(sentence)\n",
    "        if current_locations:\n",
    "            locations_memory = current_locations\n",
    "        current_dates = extract_dates(sentence)\n",
    "        if current_dates:\n",
    "            dates_memory = current_dates\n",
    "        processed_reports.extend(reports)\n",
    "    return list(set(processed_reports))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_language(text):\n",
    "    try:\n",
    "        lang = textacy.text_utils.detect_language(text)\n",
    "        return lang\n",
    "    except ValueError:\n",
    "        return 'na'\n",
    "\n",
    "def compare_reports(reports1, reports2):\n",
    "    report_hashes_1 = [r.report_hash() for r in reports1]\n",
    "    report_hashes_2 = [r.report_hash() for r in reports2]\n",
    "    equal_length = len(reports1) == len(reports2)\n",
    "    equal_contents = set(report_hashes_1) == set(report_hashes_2)\n",
    "    return equal_contents and equal_length\n",
    "\n",
    "    \n",
    "def generate_report(report_dict, article):\n",
    "    report = Report(report_dict['Location'], report_dict['DateTime'], report_dict['EventTerm'], \\\n",
    "                   report_dict['SubjectTerm'], report_dict['Quantity'], article)\n",
    "    return report\n",
    "\n",
    "def compare_report_sets(expected_reports, generated_reports):\n",
    "    expected_hashes = [r.report_hash() for r in expected_reports]\n",
    "    generated_hashes = [r.report_hash() for r in generated_reports]\n",
    "    print(\"==========Reports Not Generated==========\")\n",
    "    for h, r in zip(expected_hashes, expected_reports):\n",
    "        if h not in generated_hashes:\n",
    "            r.display()\n",
    "    print(\"\\n\")\n",
    "    print(\"==========Reports Erroneously Generated==========\")\n",
    "    for h, r in zip(generated_hashes, generated_reports):\n",
    "        if h not in expected_hashes:\n",
    "            r.display()\n",
    "                \n",
    "def run_tests(test_cases):\n",
    "    cases_with_errors = []\n",
    "    for t in test_cases:\n",
    "        article = t['article']\n",
    "        expected_reports = [generate_report(r, article) for r in t['reports']]\n",
    "        generated_reports = process_article_new(article)\n",
    "        if not compare_reports(expected_reports, generated_reports):\n",
    "            cases_with_errors.append((article, expected_reports, generated_reports))\n",
    "    error_proportion = len(cases_with_errors) / len(test_cases)\n",
    "    print(\"==========Summary==========\")\n",
    "    print(\"% of cases with errors: {:.0f}%\".format(error_proportion * 100))\n",
    "    print(\"===========================\")\n",
    "    print(\"\\n\")\n",
    "    for error_case in cases_with_errors:\n",
    "        print(\"==========Article Contents==========\")\n",
    "        print(error_case[0])\n",
    "        print(\"\\n\")\n",
    "        compare_report_sets(error_case[1], error_case[2])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_cases = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article = \"Flash flooding across Afghanistan and Pakistan has left more than 160 dead and dozens stranded in one of South Asia's worst natural disasters this year, say officials.  The flooding, caused by unusually heavy rain, has left villagers stuck in remote areas without shelter, food or power.  Mountainous Afghanistan was the worst hit, with 61 people killed and approximately 500 traditional mud-brick homes washed away in more than a dozen villages in Sarobi, a rural district less than an hour from Kabul, officials said.  Floods left a village devastated in the remote eastern Afghan province of Nuristan. At least 60 homes were destroyed across three districts, said provincial spokesman Mohammad Yusufi. No one was killed.  Authorities have been unable to deliver aid to some badly affected villages by land as roads in the area are controlled by the Taliban, Yusufi added.  “We have asked the national government for help as have an overwhelming number of locals asking for assistance, but this is a Taliban-ridden area,” Yusufi said.  At least 24 people were also died in two other eastern border provinces, Khost and Nangarhar, according to local officials. More than fifty homes and shops were destroyed and thousands of acres of farmland flooded.  In Pakistan monsoon rains claimed more than 80 lives, local media reported. Houses collapsing, drowning and electrocution all pushed up the death toll, said Sindh Information Minister Sharjeel Inam Memon.  In Karachi, the commercial capital and a southern port city that is home to 18 million people, poor neighborhoods were submerged waist-deep in water and many precincts suffered long power outages. Deaths were also reported in the north and west of the country.  Additional reporting by Reuters\"\n",
    "expected_reports = []\n",
    "expected_reports.append(Report(['Afghanistan', 'Pakistan'], ['this year'], 'die', 'person', 160, '').to_json())\n",
    "expected_reports.append(Report(['Afghanistan', 'Pakistan'], ['this year'], 'strand', 'person', 'dozens', '').to_json())\n",
    "expected_reports.append(Report(['Afghanistan', 'Pakistan'], ['this year'], 'stick', 'villager', None, '').to_json())\n",
    "expected_reports.append(Report(['Sarobi'], ['this year'], 'kill', 'people', 61, '').to_json())\n",
    "expected_reports.append(Report(['Sarobi'], ['this year'], 'wash', 'home', 500, '').to_json())\n",
    "expected_reports.append(Report(['Nuristan'], ['this year'], 'destroy', 'home', 60, '').to_json())\n",
    "expected_reports.append(Report(['Khost', 'Nangarhar'], ['this year'], 'die', 'people', 24, '').to_json())\n",
    "expected_reports.append(Report(['Khost', 'Nangarhar'], ['this year'], 'destroy', 'homes and shops', 50, '').to_json())\n",
    "expected_reports.append(Report(['Pakistan'], ['this year'], 'die', 'people', 80, '').to_json())\n",
    "expected_reports.append(Report(['Pakistan'], ['this year'], 'collapse', 'house', None, '').to_json())\n",
    "d = {}\n",
    "d['article'] = article\n",
    "d['reports'] = expected_reports\n",
    "test_cases.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article = \"'Afghanistan state news agency, Bakhtar News Agency (BNA) report that at least 7 people have been killed in flash floods in Faryab Province in the north of the country. Flash floods in Baghlan Province have killed 1 person and injured around 10 others.  Flash floods struck on 08 May 2015 in Faryab Province after a period of heavy rainfall. The districts of Garyzan, Pashtunkot and Belcheragh were worst affected. BNA report that at least 7 people were killed and over 1,500 homes damaged. The Faizabada-Takhar highway have been closed to traffic and wide areas of crops and orchards have suffered damaged.  Kuwaiti News Agency (KUNA) also report that flooding struck in the Baghlan-i-Markazi district of Baghlan province, where 1 person was killed and several injured early on Saturday 09 May 2015.  “There was heavy rain in Baghlan-e-Markazi district Friday evening and the people left their houses to safer areas. It was early Saturday when a flash flood hit the area and washed away more than 500 houses,” district Governor Gohar Khan Babri told reporters in provincial capital Pul-e-Khumri, 160 km north of Kabul.'\"\n",
    "expected_reports = []\n",
    "expected_reports.append(Report(['Faryab Province'], ['08 May 2015'], 'kill', 'people', 7, '').to_json())\n",
    "expected_reports.append(Report(['Baghlan Province'], ['08 May 2015'], 'kill', 'person', 1, '').to_json())\n",
    "expected_reports.append(Report(['Garyzan', 'Pashtunkot', 'Belcheragh'], ['08 May 2015'], 'kill', 'people', 7, '').to_json())\n",
    "expected_reports.append(Report(['Garyzan', 'Pashtunkot', 'Belcheragh'], ['08 May 2015'], 'damage', 'home', '1,500', '').to_json())\n",
    "expected_reports.append(Report(['Baghlan'], ['Saturday 09 May 2015'], 'kill', 'person', 1, '').to_json())\n",
    "expected_reports.append(Report(['Baghlan-i-Markazi', 'Baghlan'], ['early Saturday'], 'wash', 'house', 500, '').to_json())\n",
    "expected_reports.append(Report(['Baghlan-e-Markazi'], ['Friday evening'], 'leave', 'people', None, '').to_json())\n",
    "d = {}\n",
    "d['article'] = article\n",
    "d['reports'] = expected_reports\n",
    "test_cases.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article = \"'ALGIERS (AA) – Hundreds of homes have been destroyed in Algeria‘s southern city of Tamanrasset following several days of torrential rainfall, a local humanitarian aid official said Wednesday.  The city was pounded by rainfall from March 19 to March 24, according to Ghanom Sudani, a member of a government-appointed humanitarian aid committee.  He added that heavy rains had destroyed as many as 400 residences.  “Hundreds of families have had to leave their homes after they were inundated with water,” Sudani told The Anadolu Agency.  www.aa.com.tr/en  Last month neighbouring Tunisia experienced heavy rainfall and flooding in Jendouba City.'\"\n",
    "expected_reports = []\n",
    "expected_reports.append(Report(['Tamanrasset'], ['March 19'], 'destroy', 'homes', 'hundreds', '').to_json())\n",
    "expected_reports.append(Report(['Tamanrasset'], ['March 19'], 'destroy', 'residence', 400, '').to_json())\n",
    "expected_reports.append(Report(['Tamanrasset'], ['March 19'], 'leave', 'families', 'hundreds', '').to_json())\n",
    "d = {}\n",
    "d['article'] = article\n",
    "d['reports'] = expected_reports\n",
    "test_cases.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article = 'Heavy rain on Monday 09 March 2015 flooded at least 3 municipalities of Luanda, the capital of Angola.  According to Angola news agency ANGOP, Luanda fire department have reported the flooding has forced at least 800 families from their homes. Later reports suggest that as many as 1,770 homes have been damaged. The municipalities of Viana, Cacuaco and Belas are said to be the worst affected.  Some streets have been completely blocked by the floods, making it difficult for the authorities to carry out full assessments of the damage. Provincial deputy governor for technical area, Agostinho da Silva, told ANGOP that the government are providing assistance to those in flood affected areas, and have set up pumps to help remove the flood water.'\n",
    "expected_reports = []\n",
    "expected_reports.append(Report(['Luanda'], ['Monday 09 March 2015'], 'force', 'family', 800, '').to_json())\n",
    "expected_reports.append(Report(['Luanda'], ['Monday 09 March 2015'], 'damage', 'home', '1,770', '').to_json())\n",
    "d = {}\n",
    "d['article'] = article\n",
    "d['reports'] = expected_reports\n",
    "test_cases.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article = 'Flooding in Albania has killed at least three people. Torrential rain caused power cuts and water outages. Schools were closed in the west and south of the country.  A 60-year-old man and his 26-year old daughter were found dead after their car was swept away by floodwaters in Lac, northwest of the capital, Tirana, late Tuesday.  A 21-year-old motorcycle driver was also found dead in Lac, while his teenage passenger was rescued.  Army troops were on standby to help emergency workers with evacuation efforts.  “The children were screaming and crying,” said one unidentified woman whose house was flooded. “I did not know what to do. We decided to put them in a room in the second floor where it is higher.”  Authorities have evacuated families from five buildings.  The flooding hindered hospital and other public services and damaged a large area of farmland.  As the bad weather continued, the number of affected areas increased throughout Wednesday.  In neighboring Greece, weather warnings were issued for nearby parts of the country.'\n",
    "expected_reports = []\n",
    "expected_reports.append(Report(['Albania'], ['late Tuesday'], 'kill', 'people', 'three', '').to_json())\n",
    "expected_reports.append(Report(['Tirana'], ['late Tuesday'], 'evacuate', 'building', 'five', '').to_json())\n",
    "expected_reports.append(Report(['Tirana'], ['late Tuesday'], 'flood', 'house', None, '').to_json())\n",
    "d = {}\n",
    "d['article'] = article\n",
    "d['reports'] = expected_reports\n",
    "test_cases.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article = \"Nineteen people are feared dead after violent storms and severe flooding swept the French Riviera, including three people who drowned in a retirement home after a river broke its banks.\"\n",
    "expected_reports = []\n",
    "expected_reports.append(Report(None, [], 'fear dead', 'people', 'nineteen', '').to_json())\n",
    "expected_reports.append(Report(None, [], 'drown', 'people', 'three', '').to_json())\n",
    "d = {}\n",
    "d['article'] = article\n",
    "d['reports'] = expected_reports\n",
    "test_cases.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article = \"More than fifty homes and shops were destroyed and thousands of acres of farmland flooded.\"\n",
    "expected_reports = []\n",
    "expected_reports.append(Report(None, [], 'destroy', 'homes and shops', 'fifty', '').to_json())\n",
    "d = {}\n",
    "d['article'] = article\n",
    "d['reports'] = expected_reports\n",
    "test_cases.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article = \"Quoting an official from the Badakhshan provincial government, Xinhua also said that the foods had damaged or destroyed more than 120 houses in the district.\"\n",
    "expected_reports = []\n",
    "expected_reports.append(Report(['Badakhshan'], [], 'destroy', 'house', 120, '').to_json())\n",
    "d = {}\n",
    "d['article'] = article\n",
    "d['reports'] = expected_reports\n",
    "test_cases.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article = article = \"The June 17 tornado whipped through Essa Township around the supper hour, leaving 100 families homeless while others had to clean up downed trees and debris.\"\n",
    "expected_reports = []\n",
    "expected_reports.append(Report(['Essa Township'], ['June 17'], 'leave homeless', 'family', 100, '').to_json())\n",
    "d = {}\n",
    "d['article'] = article\n",
    "d['reports'] = expected_reports\n",
    "test_cases.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article = \"Mountainous Afghanistan was the worst hit, with 61 people killed and approximately 500 traditional mud-brick homes washed away in more than a dozen villages in Sarobi, a rural district less than an hour from Kabul, officials said.\"\n",
    "expected_reports = []\n",
    "expected_reports.append(Report(['Sarobi'], [], 'kill', 'people', 61, '').to_json())\n",
    "expected_reports.append(Report(['Sarobi'], [], 'wash', 'home', 500, '').to_json())\n",
    "d = {}\n",
    "d['article'] = article\n",
    "d['reports'] = expected_reports\n",
    "test_cases.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article = \"Further severe weather, floods and landslides have left 14 people dead and 4 missing in southern China.  Yesterday the Chinese government said that the storms and heavy rainfall from 18 to 22 June 2014 affected nine southern provinces. 8,700 homes have been destroyed, 66,000 homes damaged and forced 337,000 people to evacuate. 42,000 hectares of crops have also been destroyed. Further heavy rainfall is forecast for the next 24 hours.\"\n",
    "expected_reports = []\n",
    "expected_reports.append(Report(['China'], [], 'leave dead', 'people', 14, '').to_json())\n",
    "expected_reports.append(Report(['China'], ['18 June 2014'], 'destroy', 'home', '8,700', '').to_json())\n",
    "expected_reports.append(Report(['China'], ['18 June 2014'], 'damage', 'home', '66,000', '').to_json())\n",
    "expected_reports.append(Report(['China'], ['18 June 2014'], 'force', 'people', '337,000', '').to_json())\n",
    "d = {}\n",
    "d['article'] = article\n",
    "d['reports'] = expected_reports\n",
    "test_cases.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article = \"It was early Saturday when a flash flood hit the area and washed away more than 500 houses\"\n",
    "expected_reports = []\n",
    "expected_reports.append(Report(None, ['early Saturday'], 'wash', 'house', 500, '').to_json())\n",
    "d = {}\n",
    "d['article'] = article\n",
    "d['reports'] = expected_reports\n",
    "test_cases.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article = \"Within hours of the storm, Dowdall had declared a state of emergency and brought in Essa Township emergency departments staff, as well Simcoe County administrators, to assist the 300 people displaced by the storm.\"\n",
    "expected_reports = []\n",
    "expected_reports.append(Report(['Essa Township', 'Simcoe County'], [], 'displace', 'people', 300, '').to_json())\n",
    "d = {}\n",
    "d['article'] = article\n",
    "d['reports'] = expected_reports\n",
    "test_cases.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article = \"BEIJING, March 31 (Xinhua) -- The Ministry of Civil Affairs has sent 1,000 tents, 2,000 sleeping bags, 2,000 folding beds and 1,000 sets of folding desks and chairs to Jianhe County in southwestern Guizhou Province after it was hit by a 5.5-magnitude earthquake on Monday morning.  No deaths have been reported, though the quake was Guizhou's biggest in terms of magnitude since 1949. More than 23,000 people have been affected and 2,536 relocated.  Provincial authorities have sent teams to help with the rescue work and allocated 1 million yuan (about 162,880 U.S. dollars) and 206 tents for disaster relief.\"\n",
    "expected_reports = []\n",
    "expected_reports.append(Report(['Guizhou Province'], ['Monday morning'], 'relocate', 'person', '2,536', '').to_json())\n",
    "expected_reports.append(Report(['Guizhou Province'], ['Monday morning'], 'affect', 'people', '23,000', '').to_json())\n",
    "d = {}\n",
    "d['article'] = article\n",
    "d['reports'] = expected_reports\n",
    "test_cases.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article = 'As many as 2,214 households have been affected by the rainstorms in Rio Grande do Sul, the Emergency Management Service reported today (Dec. 28). A total of 1,964 households were displaced. The storms hit forty municipalities.  According to the government of Rio Grande do Sul, the State Coordination for Emergency Management continues to monitor and provide assistance to the impacted municipalities and communities.  Last Saturday (26), President Rousseff flew over the region, which borders Argentina and Uruguay, and announced the provision of $6.6 million to help communities hit by the floods.  This has been the fifth flood this year in the state, and the most severe. The Quaraí river rose a record 15.28 meters. The situation got even worse with the rise of the Uruguay river.  The rainstorm has disrupted rice harvest in the municipality of Quaraí and caused the Quaraí-Artigas international bridge between Brazil and Uruguay to remain closed off for 22 hours.    Translated by Mayra Borges'\n",
    "expected_reports = []\n",
    "expected_reports.append(Report(['Rio Grande do Sul'], ['Dec. 28'], 'displace', 'household', '1,964', '').to_json())\n",
    "expected_reports.append(Report(['Rio Grande do Sul'], ['Dec. 28'], 'affect', 'household', '2,214', '').to_json())\n",
    "d = {}\n",
    "d['article'] = article\n",
    "d['reports'] = expected_reports\n",
    "test_cases.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article = \"Verified  Kampong Cham, Kratie, Stung Treng and Kandal  Description  Due to high intensity of rainfall, Mekong River has swell and caused flooding to the surrounding areas. More flooding is expected if the rain continues. The provinces affected so far includes: Kampong Cham, Kratie, Stung Treng and Kandal12 out of Cambodia's 25 cities and provinces are suffering from floods caused by monsoon rains and Mekong River floodingIMPACT45 dead16,000 families were affected and evacuated3,080 houses inundated44,069 hectares of rice field were inundated5,617 hectares of secondary crops were inundatedRESPONSEThe local authorities provided response to the affected communities. More impact assessment is still conducted by provincial and national authorities.The government also prepared 200 units of heavy equipment in Phnom Penh and the provinces of Takeo, Svay Rieng, Oddar Meanchey and Battambang to divert water or mitigate overflows from inundated homes and farmland\"\n",
    "expected_reports = []\n",
    "expected_reports.append(Report(['Cambodia'], [], 'inundate', 'house', '3,080', '').to_json())\n",
    "expected_reports.append(Report(['Cambodia'], [], 'evacuate', 'family', '16,000', '').to_json())\n",
    "d = {}\n",
    "d['article'] = article\n",
    "d['reports'] = expected_reports\n",
    "test_cases.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article = \"No one was killed.\"\n",
    "expected_reports = []\n",
    "d = {}\n",
    "d['article'] = article\n",
    "d['reports'] = expected_reports\n",
    "test_cases.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location: ['Tamanrasset']  DateTime: ['March 19']  EventTerm: destroy  SubjectTerm:  residence  Quantity: 400\n"
     ]
    }
   ],
   "source": [
    "reports = process_article_new(\"'ALGIERS (AA) – Hundreds of homes have been destroyed in Algeria‘s southern city of Tamanrasset following several days of torrential rainfall, a local humanitarian aid official said Wednesday.  The city was pounded by rainfall from March 19 to March 24, according to Ghanom Sudani, a member of a government-appointed humanitarian aid committee.  He added that heavy rains had destroyed as many as 400 residences.  “Hundreds of families have had to leave their homes after they were inundated with water,” Sudani told The Anadolu Agency.  www.aa.com.tr/en  Last month neighbouring Tunisia experienced heavy rainfall and flooding in Jendouba City.'\")\n",
    "for r in reports:\n",
    "    r.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Summary==========\n",
      "% of cases with errors: 59%\n",
      "===========================\n",
      "\n",
      "\n",
      "==========Article Contents==========\n",
      "Flash flooding across Afghanistan and Pakistan has left more than 160 dead and dozens stranded in one of South Asia's worst natural disasters this year, say officials.  The flooding, caused by unusually heavy rain, has left villagers stuck in remote areas without shelter, food or power.  Mountainous Afghanistan was the worst hit, with 61 people killed and approximately 500 traditional mud-brick homes washed away in more than a dozen villages in Sarobi, a rural district less than an hour from Kabul, officials said.  Floods left a village devastated in the remote eastern Afghan province of Nuristan. At least 60 homes were destroyed across three districts, said provincial spokesman Mohammad Yusufi. No one was killed.  Authorities have been unable to deliver aid to some badly affected villages by land as roads in the area are controlled by the Taliban, Yusufi added.  “We have asked the national government for help as have an overwhelming number of locals asking for assistance, but this is a Taliban-ridden area,” Yusufi said.  At least 24 people were also died in two other eastern border provinces, Khost and Nangarhar, according to local officials. More than fifty homes and shops were destroyed and thousands of acres of farmland flooded.  In Pakistan monsoon rains claimed more than 80 lives, local media reported. Houses collapsing, drowning and electrocution all pushed up the death toll, said Sindh Information Minister Sharjeel Inam Memon.  In Karachi, the commercial capital and a southern port city that is home to 18 million people, poor neighborhoods were submerged waist-deep in water and many precincts suffered long power outages. Deaths were also reported in the north and west of the country.  Additional reporting by Reuters\n",
      "\n",
      "\n",
      "==========Reports Not Generated==========\n",
      "Location: ['Afghanistan', 'Pakistan']  DateTime: ['this year']  EventTerm: die  SubjectTerm:  person  Quantity: 160\n",
      "Location: ['Khost', 'Nangarhar']  DateTime: ['this year']  EventTerm: destroy  SubjectTerm:  homes and shops  Quantity: 50\n",
      "Location: ['Pakistan']  DateTime: ['this year']  EventTerm: die  SubjectTerm:  people  Quantity: 80\n",
      "\n",
      "\n",
      "==========Reports Erroneously Generated==========\n",
      "Location: ['Khost', 'Nangarhar']  DateTime: ['this year']  EventTerm: destroy  SubjectTerm:  homes and shops  Quantity: None\n",
      "\n",
      "\n",
      "==========Article Contents==========\n",
      "'Afghanistan state news agency, Bakhtar News Agency (BNA) report that at least 7 people have been killed in flash floods in Faryab Province in the north of the country. Flash floods in Baghlan Province have killed 1 person and injured around 10 others.  Flash floods struck on 08 May 2015 in Faryab Province after a period of heavy rainfall. The districts of Garyzan, Pashtunkot and Belcheragh were worst affected. BNA report that at least 7 people were killed and over 1,500 homes damaged. The Faizabada-Takhar highway have been closed to traffic and wide areas of crops and orchards have suffered damaged.  Kuwaiti News Agency (KUNA) also report that flooding struck in the Baghlan-i-Markazi district of Baghlan province, where 1 person was killed and several injured early on Saturday 09 May 2015.  “There was heavy rain in Baghlan-e-Markazi district Friday evening and the people left their houses to safer areas. It was early Saturday when a flash flood hit the area and washed away more than 500 houses,” district Governor Gohar Khan Babri told reporters in provincial capital Pul-e-Khumri, 160 km north of Kabul.'\n",
      "\n",
      "\n",
      "==========Reports Not Generated==========\n",
      "Location: ['Faryab Province']  DateTime: ['08 May 2015']  EventTerm: kill  SubjectTerm:  people  Quantity: 7\n",
      "Location: ['Baghlan Province']  DateTime: ['08 May 2015']  EventTerm: kill  SubjectTerm:  person  Quantity: 1\n",
      "Location: ['Baghlan-i-Markazi', 'Baghlan']  DateTime: ['early Saturday']  EventTerm: wash  SubjectTerm:  house  Quantity: 500\n",
      "Location: ['Baghlan-e-Markazi']  DateTime: ['Friday evening']  EventTerm: leave  SubjectTerm:  people  Quantity: None\n",
      "\n",
      "\n",
      "==========Reports Erroneously Generated==========\n",
      "Location: ['Faryab Province']  DateTime: []  EventTerm: kill  SubjectTerm:  people  Quantity: 7\n",
      "Location: ['Baghlan-e-Markazi']  DateTime: ['early Saturday']  EventTerm: wash  SubjectTerm:  house  Quantity: 500\n",
      "Location: ['Baghlan Province']  DateTime: []  EventTerm: kill  SubjectTerm:  person  Quantity: 1\n",
      "\n",
      "\n",
      "==========Article Contents==========\n",
      "'ALGIERS (AA) – Hundreds of homes have been destroyed in Algeria‘s southern city of Tamanrasset following several days of torrential rainfall, a local humanitarian aid official said Wednesday.  The city was pounded by rainfall from March 19 to March 24, according to Ghanom Sudani, a member of a government-appointed humanitarian aid committee.  He added that heavy rains had destroyed as many as 400 residences.  “Hundreds of families have had to leave their homes after they were inundated with water,” Sudani told The Anadolu Agency.  www.aa.com.tr/en  Last month neighbouring Tunisia experienced heavy rainfall and flooding in Jendouba City.'\n",
      "\n",
      "\n",
      "==========Reports Not Generated==========\n",
      "Location: ['Tamanrasset']  DateTime: ['March 19']  EventTerm: destroy  SubjectTerm:  homes  Quantity: hundreds\n",
      "Location: ['Tamanrasset']  DateTime: ['March 19']  EventTerm: leave  SubjectTerm:  families  Quantity: hundreds\n",
      "\n",
      "\n",
      "==========Reports Erroneously Generated==========\n",
      "\n",
      "\n",
      "==========Article Contents==========\n",
      "Flooding in Albania has killed at least three people. Torrential rain caused power cuts and water outages. Schools were closed in the west and south of the country.  A 60-year-old man and his 26-year old daughter were found dead after their car was swept away by floodwaters in Lac, northwest of the capital, Tirana, late Tuesday.  A 21-year-old motorcycle driver was also found dead in Lac, while his teenage passenger was rescued.  Army troops were on standby to help emergency workers with evacuation efforts.  “The children were screaming and crying,” said one unidentified woman whose house was flooded. “I did not know what to do. We decided to put them in a room in the second floor where it is higher.”  Authorities have evacuated families from five buildings.  The flooding hindered hospital and other public services and damaged a large area of farmland.  As the bad weather continued, the number of affected areas increased throughout Wednesday.  In neighboring Greece, weather warnings were issued for nearby parts of the country.\n",
      "\n",
      "\n",
      "==========Reports Not Generated==========\n",
      "Location: ['Albania']  DateTime: ['late Tuesday']  EventTerm: kill  SubjectTerm:  people  Quantity: three\n",
      "\n",
      "\n",
      "==========Reports Erroneously Generated==========\n",
      "Location: ['Albania']  DateTime: []  EventTerm: kill  SubjectTerm:  people  Quantity: three\n",
      "\n",
      "\n",
      "==========Article Contents==========\n",
      "Nineteen people are feared dead after violent storms and severe flooding swept the French Riviera, including three people who drowned in a retirement home after a river broke its banks.\n",
      "\n",
      "\n",
      "==========Reports Not Generated==========\n",
      "Location: None  DateTime: []  EventTerm: fear dead  SubjectTerm:  people  Quantity: nineteen\n",
      "\n",
      "\n",
      "==========Reports Erroneously Generated==========\n",
      "Location: None  DateTime: []  EventTerm: fear dead  SubjectTerm:  people  Quantity: None\n",
      "\n",
      "\n",
      "==========Article Contents==========\n",
      "Quoting an official from the Badakhshan provincial government, Xinhua also said that the foods had damaged or destroyed more than 120 houses in the district.\n",
      "\n",
      "\n",
      "==========Reports Not Generated==========\n",
      "Location: ['Badakhshan']  DateTime: []  EventTerm: destroy  SubjectTerm:  house  Quantity: 120\n",
      "\n",
      "\n",
      "==========Reports Erroneously Generated==========\n",
      "Location: None  DateTime: []  EventTerm: destroy  SubjectTerm:  house  Quantity: 120\n",
      "\n",
      "\n",
      "==========Article Contents==========\n",
      "Further severe weather, floods and landslides have left 14 people dead and 4 missing in southern China.  Yesterday the Chinese government said that the storms and heavy rainfall from 18 to 22 June 2014 affected nine southern provinces. 8,700 homes have been destroyed, 66,000 homes damaged and forced 337,000 people to evacuate. 42,000 hectares of crops have also been destroyed. Further heavy rainfall is forecast for the next 24 hours.\n",
      "\n",
      "\n",
      "==========Reports Not Generated==========\n",
      "Location: ['China']  DateTime: ['18 June 2014']  EventTerm: destroy  SubjectTerm:  home  Quantity: 8,700\n",
      "Location: ['China']  DateTime: ['18 June 2014']  EventTerm: damage  SubjectTerm:  home  Quantity: 66,000\n",
      "Location: ['China']  DateTime: ['18 June 2014']  EventTerm: force  SubjectTerm:  people  Quantity: 337,000\n",
      "\n",
      "\n",
      "==========Reports Erroneously Generated==========\n",
      "Location: ['China']  DateTime: ['Yesterday']  EventTerm: force  SubjectTerm:  people  Quantity: 337,000\n",
      "Location: ['China']  DateTime: ['Yesterday']  EventTerm: damage  SubjectTerm:  home  Quantity: 66,000\n",
      "Location: ['China']  DateTime: ['Yesterday']  EventTerm: destroy  SubjectTerm:  home  Quantity: 8,700\n",
      "\n",
      "\n",
      "==========Article Contents==========\n",
      "BEIJING, March 31 (Xinhua) -- The Ministry of Civil Affairs has sent 1,000 tents, 2,000 sleeping bags, 2,000 folding beds and 1,000 sets of folding desks and chairs to Jianhe County in southwestern Guizhou Province after it was hit by a 5.5-magnitude earthquake on Monday morning.  No deaths have been reported, though the quake was Guizhou's biggest in terms of magnitude since 1949. More than 23,000 people have been affected and 2,536 relocated.  Provincial authorities have sent teams to help with the rescue work and allocated 1 million yuan (about 162,880 U.S. dollars) and 206 tents for disaster relief.\n",
      "\n",
      "\n",
      "==========Reports Not Generated==========\n",
      "Location: ['Guizhou Province']  DateTime: ['Monday morning']  EventTerm: relocate  SubjectTerm:  person  Quantity: 2,536\n",
      "Location: ['Guizhou Province']  DateTime: ['Monday morning']  EventTerm: affect  SubjectTerm:  people  Quantity: 23,000\n",
      "\n",
      "\n",
      "==========Reports Erroneously Generated==========\n",
      "Location: ['Guizhou']  DateTime: ['1949']  EventTerm: relocate  SubjectTerm:  person  Quantity: 2,536\n",
      "Location: ['Guizhou']  DateTime: ['1949']  EventTerm: affect  SubjectTerm:  people  Quantity: 23,000\n",
      "\n",
      "\n",
      "==========Article Contents==========\n",
      "As many as 2,214 households have been affected by the rainstorms in Rio Grande do Sul, the Emergency Management Service reported today (Dec. 28). A total of 1,964 households were displaced. The storms hit forty municipalities.  According to the government of Rio Grande do Sul, the State Coordination for Emergency Management continues to monitor and provide assistance to the impacted municipalities and communities.  Last Saturday (26), President Rousseff flew over the region, which borders Argentina and Uruguay, and announced the provision of $6.6 million to help communities hit by the floods.  This has been the fifth flood this year in the state, and the most severe. The Quaraí river rose a record 15.28 meters. The situation got even worse with the rise of the Uruguay river.  The rainstorm has disrupted rice harvest in the municipality of Quaraí and caused the Quaraí-Artigas international bridge between Brazil and Uruguay to remain closed off for 22 hours.    Translated by Mayra Borges\n",
      "\n",
      "\n",
      "==========Reports Not Generated==========\n",
      "Location: ['Rio Grande do Sul']  DateTime: ['Dec. 28']  EventTerm: displace  SubjectTerm:  household  Quantity: 1,964\n",
      "Location: ['Rio Grande do Sul']  DateTime: ['Dec. 28']  EventTerm: affect  SubjectTerm:  household  Quantity: 2,214\n",
      "\n",
      "\n",
      "==========Reports Erroneously Generated==========\n",
      "Location: None  DateTime: ['today', 'Dec. 28']  EventTerm: displace  SubjectTerm:  household  Quantity: 1,964\n",
      "Location: None  DateTime: ['today', 'Dec. 28']  EventTerm: affect  SubjectTerm:  household  Quantity: 2,214\n",
      "\n",
      "\n",
      "==========Article Contents==========\n",
      "Verified  Kampong Cham, Kratie, Stung Treng and Kandal  Description  Due to high intensity of rainfall, Mekong River has swell and caused flooding to the surrounding areas. More flooding is expected if the rain continues. The provinces affected so far includes: Kampong Cham, Kratie, Stung Treng and Kandal12 out of Cambodia's 25 cities and provinces are suffering from floods caused by monsoon rains and Mekong River floodingIMPACT45 dead16,000 families were affected and evacuated3,080 houses inundated44,069 hectares of rice field were inundated5,617 hectares of secondary crops were inundatedRESPONSEThe local authorities provided response to the affected communities. More impact assessment is still conducted by provincial and national authorities.The government also prepared 200 units of heavy equipment in Phnom Penh and the provinces of Takeo, Svay Rieng, Oddar Meanchey and Battambang to divert water or mitigate overflows from inundated homes and farmland\n",
      "\n",
      "\n",
      "==========Reports Not Generated==========\n",
      "Location: ['Cambodia']  DateTime: []  EventTerm: evacuate  SubjectTerm:  family  Quantity: 16,000\n",
      "\n",
      "\n",
      "==========Reports Erroneously Generated==========\n",
      "Location: ['Cambodia']  DateTime: []  EventTerm: affect  SubjectTerm:  family  Quantity: 16,000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_tests(test_cases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
