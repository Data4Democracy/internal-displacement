{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Ongoing develpment of fact extraction methods **\n",
    "** Continue to focus on quality of extracted reports**\n",
    "** This is very much a work in progress. **\n",
    "\n",
    "In some cases, current reports being extracted are too naive, for example:\n",
    "\n",
    "1. \"It was early Saturday when a flash flood hit the area and washed away more than 500 houses\", results in:\n",
    "    - Location: ['Baghlan-e-Markazi']  DateTime: ['Friday']  EventTerm: flood  SubjectTerm:  house  Quantity: 500\n",
    "    - Location: ['Baghlan-e-Markazi']  DateTime: ['Friday']  EventTerm: wash  SubjectTerm:  house  Quantity: 500\n",
    "2. \"No one was killed.\", results in:\n",
    "    - Location: ['Nuristan']  DateTime: ['this year']  EventTerm: kill  SubjectTerm:  Person  Quantity: one\n",
    "3. \"More than fifty homes and shops were destroyed and thousands of acres of farmland flooded.\", results in:\n",
    "    - Location: ['Khost', 'Nangarhar']  DateTime: ['this year']  EventTerm: destroy  SubjectTerm:  home  Quantity: fifty\n",
    "    - Location: ['Khost', 'Nangarhar']  DateTime: ['this year']  EventTerm: destroy  SubjectTerm:  shop  Quantity: fifty\n",
    "    - Location: ['Khost', 'Nangarhar']  DateTime: ['this year']  EventTerm: flood  SubjectTerm:  home  Quantity: fifty\n",
    "    - Location: ['Khost', 'Nangarhar']  DateTime: ['this year']  EventTerm: flood  SubjectTerm:  shop  Quantity: fifty\n",
    "\n",
    "\n",
    "This notebook contains a new approach to fact extraction of reporting terms, reporting units and quantities by looking at the parts of speech and parse tree dependencies between the term and unit, ensuring that the relevant unit is either the subject or object of the sentence.\n",
    "\n",
    "This analysis takes advantage of the textacy library which provides a wrapper around spacy, and exposes some useful additional functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import sys\n",
    "from nltk import Tree\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from internal_displacement.pipeline import SQLArticleInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = SQLArticleInterface(\"../sql_db.sqlite\") #Connecting to pre-populated database.\n",
    "labels,features = pipeline.get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "person_reporting_terms = [\n",
    "    'displaced', 'evacuated', 'forced','flee', 'homeless', 'relief camp',\n",
    "    'sheltered', 'relocated', 'stranded','stuck','stranded',\"killed\",\"dead\",\"died\"\n",
    "]\n",
    "\n",
    "structure_reporting_terms = [\n",
    "    'destroyed','damaged','swept','collapsed','flooded','washed'\n",
    "]\n",
    "\n",
    "person_reporting_units = [\"families\",\"person\",\"people\",\"individuals\",\"locals\",\"villagers\",\"residents\",\"occupants\",\"citizens\"]\n",
    "\n",
    "structure_reporting_units = [\"home\",\"house\",\"hut\",\"dwelling\",\"building\",\"shop\",\"business\",\"apartment\",\"flat\",\"residence\"]\n",
    "\n",
    "\n",
    "person_term_lemmas = [t.lemma_ for t in nlp(\" \".join(person_reporting_terms))]\n",
    "structure_term_lemmas = [t.lemma_ for t in nlp(\" \".join(structure_reporting_terms))]\n",
    "person_unit_lemmas = [t.lemma_ for t in nlp(\" \".join(person_reporting_units))]\n",
    "structure_unit_lemmas = [t.lemma_ for t in nlp(\" \".join(structure_reporting_units))]\n",
    "\n",
    "reporting_term_lemmas = person_term_lemmas + structure_term_lemmas\n",
    "reporting_unit_lemmas = person_unit_lemmas + structure_unit_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Report:\n",
    "    def __init__(self,locations,date_times,event_term,subject_term,quantity,story):\n",
    "        self.locations = locations\n",
    "        if date_times:\n",
    "            self.date_times = [date for date in date_times]\n",
    "        else:\n",
    "            self.date_times = []\n",
    "        self.event_term = event_term\n",
    "        self.subject_term = subject_term\n",
    "        self.quantity = quantity\n",
    "        self.story = story\n",
    "    \n",
    "    def display(self):\n",
    "        print(\"Location: {}  DateTime: {}  EventTerm: {}  SubjectTerm:  {}  Quantity: {}\"\n",
    "              .format(self.locations,self.date_times,self.event_term,self.subject_term,self.quantity))\n",
    "        \n",
    "    def show_story_tree(self):\n",
    "        self.display()\n",
    "        for sentence in nlp(self.story).sents:\n",
    "            for token in sentence:\n",
    "                if token.lemma_ == self.event_term:\n",
    "                    return to_nltk_tree(sentence.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_token_equality(token_a,token_b):\n",
    "    if token_a.text == token_b.text:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def check_if_collection_contains_token(token,collection):\n",
    "    if any([test_token_equality(token,t) for t in collection]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_descendents(sentence,root=None):\n",
    "    \"\"\"\n",
    "    Retrieves all tokens that are descended from the specified root token.\n",
    "    param: root: the root token\n",
    "    param: sentence: a span from which to retrieve tokens.\n",
    "    returns: a list of tokens\n",
    "    \"\"\"\n",
    "    if not root:\n",
    "        root = sentence.root\n",
    "    return [t for t in sentence if root.is_ancestor_of(t)]\n",
    "\n",
    "def get_head_descendents(sentence,root=None):\n",
    "    \"\"\"\n",
    "    Retrieves all tokens that are descended from the head of the specified root token.\n",
    "    param: root: the root token\n",
    "    param: sentence: a span from which to retrieve tokens.\n",
    "    returns: a list of tokens\n",
    "    \"\"\"\n",
    "    if not root:\n",
    "        root = sentence.root\n",
    "    else:\n",
    "        root = root.head\n",
    "    return [t for t in sentence if root.is_ancestor_of(t)]\n",
    "    \n",
    "def check_if_entity_contains_token(tokens,entity):\n",
    "    \"\"\"\n",
    "    Function to test if a given entity contains at least one of a list of tokens.\n",
    "    param: tokens: A list of tokens\n",
    "    param: entity: A span\n",
    "    \n",
    "    returns: Boolean\n",
    "    \"\"\"\n",
    "    tokens_ = [t.text for t in tokens]\n",
    "    ret = False\n",
    "    for token in entity:\n",
    "        if token.text in tokens_:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "def get_distance_from_root(token,root):\n",
    "    \"\"\"\n",
    "    Gets the parse tree distance between a token and the sentence root.\n",
    "    :param token: a token\n",
    "    :param root: the root token of the sentence\n",
    "    \n",
    "    returns: an integer distance\n",
    "    \"\"\"\n",
    "    if token == root:\n",
    "        return 0\n",
    "    d = 1\n",
    "    p = token.head\n",
    "    while p is not root:\n",
    "        d+=1\n",
    "        p = p.head\n",
    "    return d\n",
    "\n",
    "\n",
    "def get_common_ancestors(tokens):\n",
    "    ancestors = [set(t.ancestors) for t in tokens]\n",
    "    if len(ancestors) == 0:\n",
    "        return []\n",
    "    common_ancestors = ancestors[0].intersection(*ancestors)\n",
    "    return common_ancestors    \n",
    "\n",
    "\n",
    "def get_distance_between_tokens(token_a,token_b):\n",
    "\n",
    "    if token_b in token_a.subtree:\n",
    "        distance = get_distance_from_root(token_b,token_a)\n",
    "    elif token_a in token_b.subtree:\n",
    "        distance = get_distance_from_root(token_a,token_b)\n",
    "    else:\n",
    "        common_ancestors = get_common_ancestors([token_a,token_b])\n",
    "        distance = 10000\n",
    "        for ca in common_ancestors:\n",
    "            distance_a = get_distance_from_root(ca,token_a)\n",
    "            distance_b = get_distance_from_root(ca,token_b)\n",
    "            distance_ab = distance_a + distance_b\n",
    "            if distance_ab < distance:\n",
    "                distance = distance_ab\n",
    "    return distance\n",
    "\n",
    "\n",
    "def get_closest_contiguous_location_block(entity_list,root_node):\n",
    "    location_entity_tokens = [[token for token in sentence] for sentence in entity_list]\n",
    "    token_list =  [item for sublist in location_entity_tokens for item in sublist]\n",
    "    location_tokens_by_distance = sorted([(token,get_distance_between_tokens(token,root_node)) \n",
    "                                          for token in token_list],key= lambda x: x[1])\n",
    "    closest_location = location_tokens_by_distance[0]\n",
    "    contiguous_block = [closest_token]\n",
    "    added_tokens = 1\n",
    "    while added_tokens > 0:\n",
    "        contiguous_block_ancestors = [[token for token in token_list if token.is_ancestor_of(toke)] for toke in contiguous_block ]\n",
    "        contiguous_block_subtrees = [token.subtree for token in contiguous_block]\n",
    "        contiguous_block_neighbours = contiguous_block_ancestors + contiguous_block_subtrees\n",
    "        contiguous_block_neighbours = [item for sublist in contiguous_block_neighbours for item in sublist]\n",
    "        added_tokens = 0\n",
    "        for toke in token_list:\n",
    "            if not check_if_collection_contains_token(toke,contiguous_block):\n",
    "                if toke in contiguous_block_neighbours:\n",
    "                    added_tokens +=1\n",
    "                    contiguous_block.append(toke)\n",
    "    return contiguous_block\n",
    "\n",
    "\n",
    "\n",
    "def get_contiguous_tokens(token_list):\n",
    "    common_ancestor_tokens = get_common_ancestors(token_list)\n",
    "    highest_contiguous_block = []\n",
    "    for toke in token_list:\n",
    "        if check_if_collection_contains_token(toke.head,common_ancestor_tokens):\n",
    "            highest_contiguous_block.append(toke)\n",
    "    added_tokens = 1\n",
    "    while added_tokens > 0:\n",
    "        added_tokens = 0\n",
    "        for toke in token_list:\n",
    "            if check_if_collection_contains_token(toke.head,highest_contiguous_block):\n",
    "                if not check_if_collection_contains_token(toke,highest_contiguous_block):\n",
    "                    highest_contiguous_block.append(toke)\n",
    "                    added_tokens +=1\n",
    "    return highest_contiguous_block\n",
    "\n",
    "def match_entities_in_block(entities,token_block):\n",
    "    matched = []\n",
    "    text_block = [t.text for t in token_block] #For some reason comparing identity on tokens does not always work.\n",
    "    for e in entities:\n",
    "        et = [t.text for t in e]\n",
    "        et_in_b = [t for t in et if t in text_block]\n",
    "        if len(et_in_b) == len(et):\n",
    "            matched.append(e)\n",
    "    return matched\n",
    "\n",
    "def extract_locations(sentence,root=None):\n",
    "    \"\"\"\n",
    "    Examines a sentence and identifies if any of its constituent tokens describe a location.\n",
    "    If a root token is specified, only location tokens below the level of this token in the tree will be examined. \n",
    "    If no root is specified, location tokens will be drawn from the entirety of the span.\n",
    "    param: sentence       a span\n",
    "    param: root           a token\n",
    "    returns: A list of strings, or None\n",
    "    \"\"\"\n",
    "    if not root:\n",
    "        root = sentence.root\n",
    "    descendents = get_descendents(sentence,root)\n",
    "    location_entities = [e for e in nlp(sentence.text).ents if e.label_ == \"GPE\"]\n",
    "    if len(location_entities) > 0:\n",
    "        descendent_location_tokens = []\n",
    "        for location_ent in location_entities:\n",
    "            if check_if_entity_contains_token(location_ent,descendents):\n",
    "                descendent_location_tokens.extend([token for token in location_ent])\n",
    "        contiguous_token_block = get_contiguous_tokens(descendent_location_tokens)\n",
    "\n",
    "        block_locations = match_entities_in_block(location_entities,contiguous_token_block)\n",
    "        if len(block_locations) > 0:\n",
    "            return [location.text for location in block_locations]\n",
    "        else:\n",
    "            return location_entities #If we cannot decide which one is correct, choose them all\n",
    "                                    #and figure it out at the report merging stage.\n",
    "    else:\n",
    "        return []\n",
    "def extract_dates(sentence,root=None):\n",
    "    \"\"\"\n",
    "    Examines a sentence and identifies if any of its constituent tokens describe a date.\n",
    "    If a root token is specified, only date tokens below the level of this token in the tree will be examined. \n",
    "    If no root is specified, date tokens will be drawn from the entirety of the span.\n",
    "    Unlike the extract dates function (which returns a list of strings),\n",
    "    this function returns a list of spacy spans. This is because numerical quantities detected in the \n",
    "    branch_search need to be checked to ensure they are not in fact parts of a date.\n",
    "    \n",
    "    param: sentence       a span\n",
    "    param: root           a token\n",
    "    returns: A list of spacy spans\n",
    "    \"\"\"\n",
    "    if not root:\n",
    "        root = sentence.root\n",
    "    descendents = get_head_descendents(sentence,root)\n",
    "    date_entities = [e for e in nlp(sentence.text).ents if e.label_ == \"DATE\"]\n",
    "    if len(date_entities) > 0:\n",
    "        descendent_date_tokens = []\n",
    "        for date_ent in date_entities:\n",
    "            if check_if_entity_contains_token(date_ent,descendents):\n",
    "                descendent_date_tokens.extend([token for token in date_ent])\n",
    "        contiguous_token_block = get_contiguous_tokens(descendent_date_tokens)\n",
    "\n",
    "        block_dates = match_entities_in_block(date_entities,contiguous_token_block)\n",
    "        return block_dates\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def basic_number(token):\n",
    "    if token.text == \"dozens\":\n",
    "        return True\n",
    "    if token.like_num:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_sentence_new(sentence, dates_memory, locations_memory, story):\n",
    "    \"\"\"\n",
    "    Extracts the main verbs from a sentence as a starting point\n",
    "    for report extraction.\n",
    "    \"\"\"\n",
    "    sentence_reports = []\n",
    "    # Find the verbs\n",
    "    main_verbs = textacy.spacy_utils.get_main_verbs_of_sent(sentence)\n",
    "    for v in main_verbs:\n",
    "        unit_type, verb_lemma = verb_relevance(v)\n",
    "        if unit_type:\n",
    "            reports = branch_search_new(v, verb_lemma, unit_type, dates_memory, locations_memory, sentence, story)\n",
    "            sentence_reports.extend(reports)\n",
    "    return sentence_reports\n",
    "\n",
    "def verb_relevance(verb):\n",
    "    \"\"\"\n",
    "    Checks a verb for relevance by:\n",
    "    1. Comparing to structure term lemmas\n",
    "    2. Comparing to person term lemmas\n",
    "    3. Looking for special cases such as 'leave homeless'\n",
    "    \"\"\"\n",
    "    if verb.lemma_ in structure_term_lemmas:\n",
    "        return structure_unit_lemmas, verb.lemma_\n",
    "    elif verb.lemma_ in person_term_lemmas:\n",
    "        return person_unit_lemmas, verb.lemma_\n",
    "    elif verb.lemma_ == 'leave':\n",
    "        children = verb.children\n",
    "        obj_predicate = None\n",
    "        for child in children:\n",
    "            if child.dep_ == 'oprd':\n",
    "                obj_predicate = child\n",
    "        if obj_predicate:\n",
    "            if obj_predicate.lemma_ in structure_term_lemmas:\n",
    "                return structure_unit_lemmas, 'leave ' + obj_predicate.lemma_\n",
    "            elif obj_predicate.lemma_ in person_term_lemmas:\n",
    "                return person_unit_lemmas, 'leave ' + obj_predicate.lemma_\n",
    "    return None, None\n",
    "\n",
    "def get_quantity_from_phrase(phrase):\n",
    "    \"\"\"\n",
    "    Look for number-like tokens within noun phrase.\n",
    "    \"\"\"\n",
    "    for token in phrase:\n",
    "        if basic_number(token):\n",
    "            return token\n",
    "            \n",
    "def get_quantity(sentence, unit):\n",
    "    \"\"\"\n",
    "    Split a sentence into noun phrases.\n",
    "    Search for quantities within each noun phrase.\n",
    "    If the noun phrase is part of a conjunction, then\n",
    "    search for quantity within preceding noun phrase\n",
    "    \"\"\"\n",
    "    noun_phrases = list(nlp(sentence.text).noun_chunks)\n",
    "    # Case one - see if phrase contains the unit\n",
    "    for i, np in enumerate(noun_phrases):\n",
    "        if unit.text in np.text:\n",
    "            if unit.dep_ == 'conj':\n",
    "                return get_quantity_from_phrase(noun_phrases[i-1])\n",
    "            else:\n",
    "                return get_quantity_from_phrase(np)\n",
    "\n",
    "def get_subjects_and_objects(story, verb):\n",
    "    \"\"\"\n",
    "    Identify subjects and objects for a verb\n",
    "    Also check if a reporting unit directly precedes\n",
    "    a verb and is a direct or prepositional object\n",
    "    \"\"\"\n",
    "    verb_objects = textacy.spacy_utils.get_objects_of_verb(verb)\n",
    "    verb_subjects = textacy.spacy_utils.get_subjects_of_verb(verb)\n",
    "    verb_objects.extend(verb_subjects)\n",
    "    #see if unit directly precedes verb\n",
    "    if verb.i > 0:\n",
    "        preceding = story[verb.i - 1]\n",
    "        if preceding.dep_ in ('pobj', 'dobj') and preceding not in verb_objects:\n",
    "            verb_objects.append(preceding)\n",
    "    return verb_objects\n",
    "            \n",
    "def branch_search_new(verb, verb_lemma, search_type, dates_memory, locations_memory, sentence, story):\n",
    "    \"\"\"\n",
    "    Extract reports based upon an identified verb (reporting term).\n",
    "    Extract possible locations or use most recent locations\n",
    "    Extract possible dates or use most recent dates\n",
    "    Identify reporting unit by looking in objects and subjects of reporting term (verb)\n",
    "    Identify quantity by looking in noun phrases.\n",
    "    \"\"\"\n",
    "    possible_locations = extract_locations(sentence)\n",
    "    possible_dates = extract_dates(sentence)\n",
    "    if not possible_locations:\n",
    "        possible_locations = locations_memory\n",
    "    if not possible_dates:\n",
    "        possible_dates = dates_memory\n",
    "\n",
    "    reports = []\n",
    "    quantity = None\n",
    "    verb_objects = get_subjects_and_objects(story, verb)\n",
    "    for o in verb_objects:\n",
    "        if o.lemma_ in search_type:\n",
    "            # Try and get a number\n",
    "            quantity = get_quantity(sentence, o)\n",
    "            report = Report(possible_locations, possible_dates, verb_lemma,\n",
    "                                    o.lemma_, quantity, story.text)\n",
    "            reports.append(report)\n",
    "            #report.display()\n",
    "    return reports\n",
    "\n",
    "def process_article_new(story):\n",
    "    \"\"\"\n",
    "    Process a story once sentence at a time\n",
    "    \"\"\"\n",
    "    processed_reports = []\n",
    "    #if len(story) < 25:\n",
    "    #    return processed_reports\n",
    "    story = nlp(story)\n",
    "    sentences = list(story.sents) # Split into sentences\n",
    "    dates_memory = None # Keep a running track of the most recent dates found in articles\n",
    "    locations_memory = None # Keep a running track of the most recent locations found in articles\n",
    "    for sentence in sentences: # Process sentence\n",
    "        reports = []\n",
    "        reports = process_sentence_new(sentence, dates_memory, locations_memory, story)\n",
    "        current_locations = extract_locations(sentence)\n",
    "        if current_locations:\n",
    "            locations_memory = current_locations\n",
    "        current_dates = extract_dates(sentence)\n",
    "        if current_dates:\n",
    "            dates_memory = current_dates\n",
    "        processed_reports.extend(reports)\n",
    "    return list(set(processed_reports))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location: None  DateTime: [early Saturday]  EventTerm: wash  SubjectTerm:  house  Quantity: 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<__main__.Report at 0x11ee71940>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = \"It was early Saturday when a flash flood hit the area and washed away more than 500 houses\"\n",
    "process_article_new(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location: None  DateTime: []  EventTerm: destroy  SubjectTerm:  home  Quantity: fifty\n",
      "Location: None  DateTime: []  EventTerm: destroy  SubjectTerm:  shop  Quantity: fifty\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<__main__.Report at 0x11ee71fd0>, <__main__.Report at 0x11ee71be0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = \"More than fifty homes and shops were destroyed and thousands of acres of farmland flooded.\"\n",
    "process_article_new(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location: None  DateTime: []  EventTerm: destroy  SubjectTerm:  house  Quantity: 120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<__main__.Report at 0x11ee71da0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = \"Quoting an official from the Badakhshan provincial government, Xinhua also said that the foods had damaged or destroyed more than 120 houses in the district.\"\n",
    "process_article_new(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location: ['Afghanistan']  DateTime: []  EventTerm: kill  SubjectTerm:  people  Quantity: 61\n",
      "Location: ['Afghanistan']  DateTime: []  EventTerm: wash  SubjectTerm:  home  Quantity: 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<__main__.Report at 0x11ee71a90>, <__main__.Report at 0x11ee71d30>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = \"Mountainous Afghanistan was the worst hit, with 61 people killed and approximately 500 traditional mud-brick homes washed away in more than a dozen villages in Sarobi, a rural district less than an hour from Kabul, officials said.\"\n",
    "process_article_new(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location: ['Essa Township']  DateTime: [June 17]  EventTerm: leave homeless  SubjectTerm:  family  Quantity: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<__main__.Report at 0x11ee71128>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = \"The June 17 tornado whipped through Essa Township around the supper hour, leaving 100 families homeless while others had to clean up downed trees and debris.\"\n",
    "process_article_new(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location: ['Essa Township']  DateTime: []  EventTerm: displace  SubjectTerm:  people  Quantity: 300\n"
     ]
    }
   ],
   "source": [
    "article = \"Within hours of the storm, Dowdall had declared a state of emergency and brought in Essa Township emergency departments staff, as well Simcoe County administrators, to assist the 300 people displaced by the storm.\"\n",
    "reports = process_article_new(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = pd.DataFrame(features, columns=['content'])\n",
    "features = features[~features['content'].isin(['', 'retrieval_failed'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features['new_reports'] = features['content'].apply(lambda x: process_article_new(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features['num_new_reports'] = features['new_reports'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fail_cases = features[features['num_new_reports'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fail_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Verified  Kampong Cham, Kratie, Stung Treng and Kandal  Description  Due to high intensity of rainfall, Mekong River has swell and caused flooding to the surrounding areas. More flooding is expected if the rain continues. The provinces affected so far includes: Kampong Cham, Kratie, Stung Treng and Kandal12 out of Cambodia's 25 cities and provinces are suffering from floods caused by monsoon rains and Mekong River floodingIMPACT45 dead16,000 families were affected and evacuated3,080 houses inundated44,069 hectares of rice field were inundated5,617 hectares of secondary crops were inundatedRESPONSEThe local authorities provided response to the affected communities. More impact assessment is still conducted by provincial and national authorities.The government also prepared 200 units of heavy equipment in Phnom Penh and the provinces of Takeo, Svay Rieng, Oddar Meanchey and Battambang to divert water or mitigate overflows from inundated homes and farmland\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fail_cases.iloc[7]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As many as 2,214 households have been affected by the rainstorms in Rio Grande do Sul, the Emergency Management Service reported today (Dec. 28). A total of 1,964 households were displaced. The storms hit forty municipalities.  According to the government of Rio Grande do Sul, the State Coordination for Emergency Management continues to monitor and provide assistance to the impacted municipalities and communities.  Last Saturday (26), President Rousseff flew over the region, which borders Argentina and Uruguay, and announced the provision of $6.6 million to help communities hit by the floods.  This has been the fifth flood this year in the state, and the most severe. The Quaraí river rose a record 15.28 meters. The situation got even worse with the rise of the Uruguay river.  The rainstorm has disrupted rice harvest in the municipality of Quaraí and caused the Quaraí-Artigas international bridge between Brazil and Uruguay to remain closed off for 22 hours.    Translated by Mayra Borges'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fail_cases.iloc[8]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"BEIJING, March 31 (Xinhua) -- The Ministry of Civil Affairs has sent 1,000 tents, 2,000 sleeping bags, 2,000 folding beds and 1,000 sets of folding desks and chairs to Jianhe County in southwestern Guizhou Province after it was hit by a 5.5-magnitude earthquake on Monday morning.  No deaths have been reported, though the quake was Guizhou's biggest in terms of magnitude since 1949. More than 23,000 people have been affected and 2,536 relocated.  Provincial authorities have sent teams to help with the rescue work and allocated 1 million yuan (about 162,880 U.S. dollars) and 206 tents for disaster relief.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fail_cases.iloc[18]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
